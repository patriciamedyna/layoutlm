{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq_labeling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patriciamedyna/layoutlm/blob/master/Seq_labeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzlDS0aOCDq-",
        "outputId": "c9b847f7-ee2f-4279-8ffa-2650dff1d578",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZ68DSY2AzQ3",
        "outputId": "062a6339-ea96-480f-d74a-6fd49049a6ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        }
      },
      "source": [
        "!pip install transformers==2.9.0 tensorboardX==2.0 lxml==4.5.1 seqeval==0.0.12 Pillow==7.1.2 flake8==3.8.2 isort==4.3.21 black==19.10b0 pre-commit==2.4.0\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==2.9.0 in /usr/local/lib/python3.6/site-packages (2.9.0)\n",
            "Requirement already satisfied: tensorboardX==2.0 in /usr/local/lib/python3.6/site-packages (2.0)\n",
            "Requirement already satisfied: lxml==4.5.1 in /usr/local/lib/python3.6/site-packages (4.5.1)\n",
            "Requirement already satisfied: seqeval==0.0.12 in /usr/local/lib/python3.6/site-packages (0.0.12)\n",
            "Requirement already satisfied: Pillow==7.1.2 in /usr/local/lib/python3.6/site-packages (7.1.2)\n",
            "Requirement already satisfied: flake8==3.8.2 in /usr/local/lib/python3.6/site-packages (3.8.2)\n",
            "Requirement already satisfied: isort==4.3.21 in /usr/local/lib/python3.6/site-packages (4.3.21)\n",
            "Requirement already satisfied: black==19.10b0 in /usr/local/lib/python3.6/site-packages (19.10b0)\n",
            "Requirement already satisfied: pre-commit==2.4.0 in /usr/local/lib/python3.6/site-packages (2.4.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/site-packages (from transformers==2.9.0) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/site-packages (from transformers==2.9.0) (2.24.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/site-packages (from transformers==2.9.0) (0.1.91)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/site-packages (from transformers==2.9.0) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/site-packages (from transformers==2.9.0) (0.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/site-packages (from transformers==2.9.0) (2020.10.11)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/site-packages (from transformers==2.9.0) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/site-packages (from transformers==2.9.0) (4.50.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (from transformers==2.9.0) (1.19.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/site-packages (from tensorboardX==2.0) (3.13.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from tensorboardX==2.0) (1.15.0)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/site-packages (from seqeval==0.0.12) (2.4.3)\n",
            "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /usr/local/lib/python3.6/site-packages (from flake8==3.8.2) (0.6.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/site-packages (from flake8==3.8.2) (2.0.0)\n",
            "Requirement already satisfied: pycodestyle<2.7.0,>=2.6.0a1 in /usr/local/lib/python3.6/site-packages (from flake8==3.8.2) (2.6.0)\n",
            "Requirement already satisfied: pyflakes<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/site-packages (from flake8==3.8.2) (2.2.0)\n",
            "Requirement already satisfied: click>=6.5 in /usr/local/lib/python3.6/site-packages (from black==19.10b0) (7.1.2)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/site-packages (from black==19.10b0) (20.2.0)\n",
            "Requirement already satisfied: pathspec<1,>=0.6 in /usr/local/lib/python3.6/site-packages (from black==19.10b0) (0.8.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.6/site-packages (from black==19.10b0) (1.4.4)\n",
            "Requirement already satisfied: typed-ast>=1.4.0 in /usr/local/lib/python3.6/site-packages (from black==19.10b0) (1.4.1)\n",
            "Requirement already satisfied: toml>=0.9.4 in /usr/local/lib/python3.6/site-packages (from black==19.10b0) (0.10.1)\n",
            "Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.6/site-packages (from pre-commit==2.4.0) (1.5.0)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.7\" in /usr/local/lib/python3.6/site-packages (from pre-commit==2.4.0) (3.0.0)\n",
            "Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.6/site-packages (from pre-commit==2.4.0) (1.5.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.6/site-packages (from pre-commit==2.4.0) (5.3.1)\n",
            "Requirement already satisfied: virtualenv>=20.0.8 in /usr/local/lib/python3.6/site-packages (from pre-commit==2.4.0) (20.0.34)\n",
            "Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.6/site-packages (from pre-commit==2.4.0) (3.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/site-packages (from sacremoses->transformers==2.9.0) (0.17.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/site-packages (from requests->transformers==2.9.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests->transformers==2.9.0) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/site-packages (from requests->transformers==2.9.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests->transformers==2.9.0) (1.25.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorboardX==2.0) (50.3.0.post20201006)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval==0.0.12) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval==0.0.12) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->flake8==3.8.2) (3.3.0)\n",
            "Requirement already satisfied: distlib<1,>=0.3.1 in /usr/local/lib/python3.6/site-packages (from virtualenv>=20.0.8->pre-commit==2.4.0) (0.3.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4x29uOMMPEEx",
        "outputId": "00cc6874-0d4e-4f99-db84-55c642fbf9d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%env PYTHONPATH="
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: PYTHONPATH=\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f-9f_xoRRwU",
        "outputId": "7267a2de-847c-4587-b420-0c3c1e993ea9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%bash\n",
        "MINICONDA_INSTALLER_SCRIPT=Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "MINICONDA_PREFIX=/usr/local\n",
        "wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT\n",
        "chmod +x $MINICONDA_INSTALLER_SCRIPT\n",
        "./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREFIX=/usr/local\n",
            "installing: python-3.6.5-hc3d631a_2 ...\n",
            "installing: ca-certificates-2018.03.07-0 ...\n",
            "installing: conda-env-2.6.0-h36134e3_1 ...\n",
            "installing: libgcc-ng-7.2.0-hdf63c60_3 ...\n",
            "installing: libstdcxx-ng-7.2.0-hdf63c60_3 ...\n",
            "installing: libffi-3.2.1-hd88cf55_4 ...\n",
            "installing: ncurses-6.1-hf484d3e_0 ...\n",
            "installing: openssl-1.0.2o-h20670df_0 ...\n",
            "installing: tk-8.6.7-hc745277_3 ...\n",
            "installing: xz-5.2.4-h14c3975_4 ...\n",
            "installing: yaml-0.1.7-had09818_2 ...\n",
            "installing: zlib-1.2.11-ha838bed_2 ...\n",
            "installing: libedit-3.1.20170329-h6b74fdf_2 ...\n",
            "installing: readline-7.0-ha6073c6_4 ...\n",
            "installing: sqlite-3.23.1-he433501_0 ...\n",
            "installing: asn1crypto-0.24.0-py36_0 ...\n",
            "installing: certifi-2018.4.16-py36_0 ...\n",
            "installing: chardet-3.0.4-py36h0f667ec_1 ...\n",
            "installing: idna-2.6-py36h82fb2a8_1 ...\n",
            "installing: pycosat-0.6.3-py36h0a5515d_0 ...\n",
            "installing: pycparser-2.18-py36hf9f622e_1 ...\n",
            "installing: pysocks-1.6.8-py36_0 ...\n",
            "installing: ruamel_yaml-0.15.37-py36h14c3975_2 ...\n",
            "installing: six-1.11.0-py36h372c433_1 ...\n",
            "installing: cffi-1.11.5-py36h9745a5d_0 ...\n",
            "installing: setuptools-39.2.0-py36_0 ...\n",
            "installing: cryptography-2.2.2-py36h14c3975_0 ...\n",
            "installing: wheel-0.31.1-py36_0 ...\n",
            "installing: pip-10.0.1-py36_0 ...\n",
            "installing: pyopenssl-18.0.0-py36_0 ...\n",
            "installing: urllib3-1.22-py36hbe7ace6_0 ...\n",
            "installing: requests-2.18.4-py36he2e5f8d_1 ...\n",
            "installing: conda-4.5.4-py36_0 ...\n",
            "unlinking: ca-certificates-2020.10.14-0\n",
            "unlinking: certifi-2020.6.20-py36_0\n",
            "unlinking: cffi-1.14.3-py36he30daa8_0\n",
            "unlinking: chardet-3.0.4-py36_1003\n",
            "unlinking: conda-4.8.5-py36_0\n",
            "unlinking: cryptography-3.1.1-py36h1ba5d50_0\n",
            "unlinking: idna-2.10-py_0\n",
            "unlinking: libedit-3.1.20191231-h14c3975_1\n",
            "unlinking: libffi-3.3-he6710b0_2\n",
            "unlinking: libgcc-ng-9.1.0-hdf63c60_0\n",
            "unlinking: libstdcxx-ng-9.1.0-hdf63c60_0\n",
            "unlinking: ncurses-6.2-he6710b0_1\n",
            "unlinking: openssl-1.1.1h-h7b6447c_0\n",
            "unlinking: pip-20.2.3-py36_0\n",
            "unlinking: pycosat-0.6.3-py36h7b6447c_0\n",
            "unlinking: pycparser-2.20-py_2\n",
            "unlinking: pyopenssl-19.1.0-py_1\n",
            "unlinking: pysocks-1.7.1-py36_0\n",
            "unlinking: python-3.6.12-hcff3b4d_2\n",
            "unlinking: readline-8.0-h7b6447c_0\n",
            "unlinking: requests-2.24.0-py_0\n",
            "unlinking: ruamel_yaml-0.15.87-py36h7b6447c_1\n",
            "unlinking: setuptools-50.3.0-py36hb0f4dca_1\n",
            "unlinking: six-1.15.0-py_0\n",
            "unlinking: sqlite-3.33.0-h62c20be_0\n",
            "unlinking: tk-8.6.10-hbc83047_0\n",
            "unlinking: urllib3-1.25.10-py_0\n",
            "unlinking: wheel-0.35.1-py_0\n",
            "unlinking: xz-5.2.5-h7b6447c_0\n",
            "unlinking: yaml-0.2.5-h7b6447c_0\n",
            "unlinking: zlib-1.2.11-h7b6447c_3\n",
            "installation finished.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "--2020-10-14 23:08:23--  https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
            "Resolving repo.continuum.io (repo.continuum.io)... 104.18.201.79, 104.18.200.79, 2606:4700::6812:c84f, ...\n",
            "Connecting to repo.continuum.io (repo.continuum.io)|104.18.201.79|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh [following]\n",
            "--2020-10-14 23:08:23--  https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.131.3, 104.16.130.3, 2606:4700::6810:8303, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.131.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 58468498 (56M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-4.5.4-Linux-x86_64.sh.1’\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  0% 13.1M 4s\n",
            "    50K .......... .......... .......... .......... ..........  0% 5.16M 8s\n",
            "   100K .......... .......... .......... .......... ..........  0% 6.58M 8s\n",
            "   150K .......... .......... .......... .......... ..........  0% 10.1M 7s\n",
            "   200K .......... .......... .......... .......... ..........  0% 7.76M 7s\n",
            "   250K .......... .......... .......... .......... ..........  0% 59.4M 6s\n",
            "   300K .......... .......... .......... .......... ..........  0% 71.4M 5s\n",
            "   350K .......... .......... .......... .......... ..........  0% 37.5M 5s\n",
            "   400K .......... .......... .......... .......... ..........  0% 34.6M 5s\n",
            "   450K .......... .......... .......... .......... ..........  0% 13.8M 4s\n",
            "   500K .......... .......... .......... .......... ..........  0% 27.0M 4s\n",
            "   550K .......... .......... .......... .......... ..........  1% 51.2M 4s\n",
            "   600K .......... .......... .......... .......... ..........  1% 68.4M 4s\n",
            "   650K .......... .......... .......... .......... ..........  1% 62.1M 4s\n",
            "   700K .......... .......... .......... .......... ..........  1% 69.2M 3s\n",
            "   750K .......... .......... .......... .......... ..........  1% 61.9M 3s\n",
            "   800K .......... .......... .......... .......... ..........  1% 73.0M 3s\n",
            "   850K .......... .......... .......... .......... ..........  1% 79.2M 3s\n",
            "   900K .......... .......... .......... .......... ..........  1% 66.2M 3s\n",
            "   950K .......... .......... .......... .......... ..........  1% 57.1M 3s\n",
            "  1000K .......... .......... .......... .......... ..........  1% 30.3M 3s\n",
            "  1050K .......... .......... .......... .......... ..........  1% 64.7M 3s\n",
            "  1100K .......... .......... .......... .......... ..........  2% 51.6M 3s\n",
            "  1150K .......... .......... .......... .......... ..........  2% 52.0M 2s\n",
            "  1200K .......... .......... .......... .......... ..........  2% 51.1M 2s\n",
            "  1250K .......... .......... .......... .......... ..........  2% 48.7M 2s\n",
            "  1300K .......... .......... .......... .......... ..........  2% 68.9M 2s\n",
            "  1350K .......... .......... .......... .......... ..........  2% 68.8M 2s\n",
            "  1400K .......... .......... .......... .......... ..........  2% 54.8M 2s\n",
            "  1450K .......... .......... .......... .......... ..........  2% 54.3M 2s\n",
            "  1500K .......... .......... .......... .......... ..........  2% 51.9M 2s\n",
            "  1550K .......... .......... .......... .......... ..........  2% 24.6M 2s\n",
            "  1600K .......... .......... .......... .......... ..........  2% 55.0M 2s\n",
            "  1650K .......... .......... .......... .......... ..........  2% 52.9M 2s\n",
            "  1700K .......... .......... .......... .......... ..........  3% 50.9M 2s\n",
            "  1750K .......... .......... .......... .......... ..........  3% 31.9M 2s\n",
            "  1800K .......... .......... .......... .......... ..........  3% 60.9M 2s\n",
            "  1850K .......... .......... .......... .......... ..........  3% 64.6M 2s\n",
            "  1900K .......... .......... .......... .......... ..........  3% 72.0M 2s\n",
            "  1950K .......... .......... .......... .......... ..........  3% 56.7M 2s\n",
            "  2000K .......... .......... .......... .......... ..........  3% 62.5M 2s\n",
            "  2050K .......... .......... .......... .......... ..........  3% 34.0M 2s\n",
            "  2100K .......... .......... .......... .......... ..........  3%  100M 2s\n",
            "  2150K .......... .......... .......... .......... ..........  3% 70.6M 2s\n",
            "  2200K .......... .......... .......... .......... ..........  3% 51.4M 2s\n",
            "  2250K .......... .......... .......... .......... ..........  4% 47.3M 2s\n",
            "  2300K .......... .......... .......... .......... ..........  4% 64.1M 2s\n",
            "  2350K .......... .......... .......... .......... ..........  4% 60.6M 2s\n",
            "  2400K .......... .......... .......... .......... ..........  4% 75.7M 2s\n",
            "  2450K .......... .......... .......... .......... ..........  4% 65.1M 2s\n",
            "  2500K .......... .......... .......... .......... ..........  4% 70.5M 2s\n",
            "  2550K .......... .......... .......... .......... ..........  4% 31.0M 2s\n",
            "  2600K .......... .......... .......... .......... ..........  4% 62.9M 2s\n",
            "  2650K .......... .......... .......... .......... ..........  4% 64.7M 2s\n",
            "  2700K .......... .......... .......... .......... ..........  4% 64.9M 2s\n",
            "  2750K .......... .......... .......... .......... ..........  4% 58.3M 2s\n",
            "  2800K .......... .......... .......... .......... ..........  4% 65.9M 2s\n",
            "  2850K .......... .......... .......... .......... ..........  5% 60.6M 2s\n",
            "  2900K .......... .......... .......... .......... ..........  5% 70.0M 2s\n",
            "  2950K .......... .......... .......... .......... ..........  5% 65.4M 2s\n",
            "  3000K .......... .......... .......... .......... ..........  5% 69.8M 2s\n",
            "  3050K .......... .......... .......... .......... ..........  5% 36.8M 2s\n",
            "  3100K .......... .......... .......... .......... ..........  5% 71.2M 1s\n",
            "  3150K .......... .......... .......... .......... ..........  5% 54.8M 1s\n",
            "  3200K .......... .......... .......... .......... ..........  5% 71.9M 1s\n",
            "  3250K .......... .......... .......... .......... ..........  5% 82.1M 1s\n",
            "  3300K .......... .......... .......... .......... ..........  5% 75.1M 1s\n",
            "  3350K .......... .......... .......... .......... ..........  5% 62.4M 1s\n",
            "  3400K .......... .......... .......... .......... ..........  6% 86.3M 1s\n",
            "  3450K .......... .......... .......... .......... ..........  6% 82.4M 1s\n",
            "  3500K .......... .......... .......... .......... ..........  6% 86.1M 1s\n",
            "  3550K .......... .......... .......... .......... ..........  6% 31.1M 1s\n",
            "  3600K .......... .......... .......... .......... ..........  6% 48.5M 1s\n",
            "  3650K .......... .......... .......... .......... ..........  6% 70.2M 1s\n",
            "  3700K .......... .......... .......... .......... ..........  6% 77.0M 1s\n",
            "  3750K .......... .......... .......... .......... ..........  6% 75.8M 1s\n",
            "  3800K .......... .......... .......... .......... ..........  6% 74.7M 1s\n",
            "  3850K .......... .......... .......... .......... ..........  6% 68.8M 1s\n",
            "  3900K .......... .......... .......... .......... ..........  6% 90.7M 1s\n",
            "  3950K .......... .......... .......... .......... ..........  7% 69.2M 1s\n",
            "  4000K .......... .......... .......... .......... ..........  7% 70.8M 1s\n",
            "  4050K .......... .......... .......... .......... ..........  7% 57.2M 1s\n",
            "  4100K .......... .......... .......... .......... ..........  7% 8.13M 1s\n",
            "  4150K .......... .......... .......... .......... ..........  7% 32.2M 1s\n",
            "  4200K .......... .......... .......... .......... ..........  7% 37.9M 1s\n",
            "  4250K .......... .......... .......... .......... ..........  7% 64.9M 1s\n",
            "  4300K .......... .......... .......... .......... ..........  7% 94.9M 1s\n",
            "  4350K .......... .......... .......... .......... ..........  7% 61.4M 1s\n",
            "  4400K .......... .......... .......... .......... ..........  7% 91.1M 1s\n",
            "  4450K .......... .......... .......... .......... ..........  7% 90.0M 1s\n",
            "  4500K .......... .......... .......... .......... ..........  7% 83.0M 1s\n",
            "  4550K .......... .......... .......... .......... ..........  8% 73.5M 1s\n",
            "  4600K .......... .......... .......... .......... ..........  8% 29.4M 1s\n",
            "  4650K .......... .......... .......... .......... ..........  8%  102M 1s\n",
            "  4700K .......... .......... .......... .......... ..........  8%  103M 1s\n",
            "  4750K .......... .......... .......... .......... ..........  8% 75.8M 1s\n",
            "  4800K .......... .......... .......... .......... ..........  8%  101M 1s\n",
            "  4850K .......... .......... .......... .......... ..........  8%  111M 1s\n",
            "  4900K .......... .......... .......... .......... ..........  8% 91.0M 1s\n",
            "  4950K .......... .......... .......... .......... ..........  8% 76.9M 1s\n",
            "  5000K .......... .......... .......... .......... ..........  8% 90.7M 1s\n",
            "  5050K .......... .......... .......... .......... ..........  8% 71.1M 1s\n",
            "  5100K .......... .......... .......... .......... ..........  9% 36.4M 1s\n",
            "  5150K .......... .......... .......... .......... ..........  9% 61.6M 1s\n",
            "  5200K .......... .......... .......... .......... ..........  9% 82.0M 1s\n",
            "  5250K .......... .......... .......... .......... ..........  9% 84.8M 1s\n",
            "  5300K .......... .......... .......... .......... ..........  9% 74.4M 1s\n",
            "  5350K .......... .......... .......... .......... ..........  9% 67.1M 1s\n",
            "  5400K .......... .......... .......... .......... ..........  9% 86.4M 1s\n",
            "  5450K .......... .......... .......... .......... ..........  9% 86.4M 1s\n",
            "  5500K .......... .......... .......... .......... ..........  9% 89.8M 1s\n",
            "  5550K .......... .......... .......... .......... ..........  9% 63.5M 1s\n",
            "  5600K .......... .......... .......... .......... ..........  9% 38.5M 1s\n",
            "  5650K .......... .......... .......... .......... ..........  9% 76.0M 1s\n",
            "  5700K .......... .......... .......... .......... .......... 10% 78.9M 1s\n",
            "  5750K .......... .......... .......... .......... .......... 10% 55.9M 1s\n",
            "  5800K .......... .......... .......... .......... .......... 10% 74.1M 1s\n",
            "  5850K .......... .......... .......... .......... .......... 10% 71.7M 1s\n",
            "  5900K .......... .......... .......... .......... .......... 10% 64.2M 1s\n",
            "  5950K .......... .......... .......... .......... .......... 10% 59.7M 1s\n",
            "  6000K .......... .......... .......... .......... .......... 10% 82.6M 1s\n",
            "  6050K .......... .......... .......... .......... .......... 10% 85.7M 1s\n",
            "  6100K .......... .......... .......... .......... .......... 10% 83.9M 1s\n",
            "  6150K .......... .......... .......... .......... .......... 10% 35.8M 1s\n",
            "  6200K .......... .......... .......... .......... .......... 10% 80.6M 1s\n",
            "  6250K .......... .......... .......... .......... .......... 11% 85.2M 1s\n",
            "  6300K .......... .......... .......... .......... .......... 11% 53.9M 1s\n",
            "  6350K .......... .......... .......... .......... .......... 11% 54.0M 1s\n",
            "  6400K .......... .......... .......... .......... .......... 11% 62.1M 1s\n",
            "  6450K .......... .......... .......... .......... .......... 11% 73.1M 1s\n",
            "  6500K .......... .......... .......... .......... .......... 11% 74.7M 1s\n",
            "  6550K .......... .......... .......... .......... .......... 11% 64.9M 1s\n",
            "  6600K .......... .......... .......... .......... .......... 11% 81.1M 1s\n",
            "  6650K .......... .......... .......... .......... .......... 11% 37.0M 1s\n",
            "  6700K .......... .......... .......... .......... .......... 11% 74.3M 1s\n",
            "  6750K .......... .......... .......... .......... .......... 11% 58.3M 1s\n",
            "  6800K .......... .......... .......... .......... .......... 11% 57.4M 1s\n",
            "  6850K .......... .......... .......... .......... .......... 12% 75.1M 1s\n",
            "  6900K .......... .......... .......... .......... .......... 12% 83.5M 1s\n",
            "  6950K .......... .......... .......... .......... .......... 12% 56.9M 1s\n",
            "  7000K .......... .......... .......... .......... .......... 12% 82.1M 1s\n",
            "  7050K .......... .......... .......... .......... .......... 12% 76.2M 1s\n",
            "  7100K .......... .......... .......... .......... .......... 12% 76.1M 1s\n",
            "  7150K .......... .......... .......... .......... .......... 12% 32.8M 1s\n",
            "  7200K .......... .......... .......... .......... .......... 12% 64.4M 1s\n",
            "  7250K .......... .......... .......... .......... .......... 12% 58.5M 1s\n",
            "  7300K .......... .......... .......... .......... .......... 12% 68.2M 1s\n",
            "  7350K .......... .......... .......... .......... .......... 12% 65.8M 1s\n",
            "  7400K .......... .......... .......... .......... .......... 13% 72.9M 1s\n",
            "  7450K .......... .......... .......... .......... .......... 13% 68.7M 1s\n",
            "  7500K .......... .......... .......... .......... .......... 13% 79.8M 1s\n",
            "  7550K .......... .......... .......... .......... .......... 13% 67.9M 1s\n",
            "  7600K .......... .......... .......... .......... .......... 13% 75.3M 1s\n",
            "  7650K .......... .......... .......... .......... .......... 13% 37.5M 1s\n",
            "  7700K .......... .......... .......... .......... .......... 13% 51.0M 1s\n",
            "  7750K .......... .......... .......... .......... .......... 13% 41.2M 1s\n",
            "  7800K .......... .......... .......... .......... .......... 13% 84.0M 1s\n",
            "  7850K .......... .......... .......... .......... .......... 13% 78.2M 1s\n",
            "  7900K .......... .......... .......... .......... .......... 13% 59.4M 1s\n",
            "  7950K .......... .......... .......... .......... .......... 14% 60.9M 1s\n",
            "  8000K .......... .......... .......... .......... .......... 14% 78.3M 1s\n",
            "  8050K .......... .......... .......... .......... .......... 14% 77.3M 1s\n",
            "  8100K .......... .......... .......... .......... .......... 14% 78.2M 1s\n",
            "  8150K .......... .......... .......... .......... .......... 14% 72.4M 1s\n",
            "  8200K .......... .......... .......... .......... .......... 14% 37.1M 1s\n",
            "  8250K .......... .......... .......... .......... .......... 14% 68.6M 1s\n",
            "  8300K .......... .......... .......... .......... .......... 14% 69.1M 1s\n",
            "  8350K .......... .......... .......... .......... .......... 14% 64.9M 1s\n",
            "  8400K .......... .......... .......... .......... .......... 14% 62.3M 1s\n",
            "  8450K .......... .......... .......... .......... .......... 14% 65.8M 1s\n",
            "  8500K .......... .......... .......... .......... .......... 14% 64.7M 1s\n",
            "  8550K .......... .......... .......... .......... .......... 15% 62.7M 1s\n",
            "  8600K .......... .......... .......... .......... .......... 15% 76.0M 1s\n",
            "  8650K .......... .......... .......... .......... .......... 15% 68.4M 1s\n",
            "  8700K .......... .......... .......... .......... .......... 15% 35.1M 1s\n",
            "  8750K .......... .......... .......... .......... .......... 15% 48.6M 1s\n",
            "  8800K .......... .......... .......... .......... .......... 15% 64.0M 1s\n",
            "  8850K .......... .......... .......... .......... .......... 15% 69.8M 1s\n",
            "  8900K .......... .......... .......... .......... .......... 15% 66.2M 1s\n",
            "  8950K .......... .......... .......... .......... .......... 15% 58.8M 1s\n",
            "  9000K .......... .......... .......... .......... .......... 15% 68.5M 1s\n",
            "  9050K .......... .......... .......... .......... .......... 15% 69.8M 1s\n",
            "  9100K .......... .......... .......... .......... .......... 16% 74.6M 1s\n",
            "  9150K .......... .......... .......... .......... .......... 16% 66.0M 1s\n",
            "  9200K .......... .......... .......... .......... .......... 16% 33.7M 1s\n",
            "  9250K .......... .......... .......... .......... .......... 16% 64.1M 1s\n",
            "  9300K .......... .......... .......... .......... .......... 16% 69.0M 1s\n",
            "  9350K .......... .......... .......... .......... .......... 16% 61.7M 1s\n",
            "  9400K .......... .......... .......... .......... .......... 16% 71.0M 1s\n",
            "  9450K .......... .......... .......... .......... .......... 16% 59.7M 1s\n",
            "  9500K .......... .......... .......... .......... .......... 16% 80.9M 1s\n",
            "  9550K .......... .......... .......... .......... .......... 16% 72.4M 1s\n",
            "  9600K .......... .......... .......... .......... .......... 16% 86.1M 1s\n",
            "  9650K .......... .......... .......... .......... .......... 16% 78.2M 1s\n",
            "  9700K .......... .......... .......... .......... .......... 17% 36.8M 1s\n",
            "  9750K .......... .......... .......... .......... .......... 17% 61.7M 1s\n",
            "  9800K .......... .......... .......... .......... .......... 17% 70.1M 1s\n",
            "  9850K .......... .......... .......... .......... .......... 17% 64.8M 1s\n",
            "  9900K .......... .......... .......... .......... .......... 17% 75.1M 1s\n",
            "  9950K .......... .......... .......... .......... .......... 17% 61.0M 1s\n",
            " 10000K .......... .......... .......... .......... .......... 17% 72.5M 1s\n",
            " 10050K .......... .......... .......... .......... .......... 17% 89.1M 1s\n",
            " 10100K .......... .......... .......... .......... .......... 17% 88.5M 1s\n",
            " 10150K .......... .......... .......... .......... .......... 17% 71.6M 1s\n",
            " 10200K .......... .......... .......... .......... .......... 17% 84.1M 1s\n",
            " 10250K .......... .......... .......... .......... .......... 18% 33.5M 1s\n",
            " 10300K .......... .......... .......... .......... .......... 18% 81.1M 1s\n",
            " 10350K .......... .......... .......... .......... .......... 18% 59.3M 1s\n",
            " 10400K .......... .......... .......... .......... .......... 18% 63.2M 1s\n",
            " 10450K .......... .......... .......... .......... .......... 18% 69.7M 1s\n",
            " 10500K .......... .......... .......... .......... .......... 18% 79.5M 1s\n",
            " 10550K .......... .......... .......... .......... .......... 18% 68.0M 1s\n",
            " 10600K .......... .......... .......... .......... .......... 18% 92.2M 1s\n",
            " 10650K .......... .......... .......... .......... .......... 18% 83.0M 1s\n",
            " 10700K .......... .......... .......... .......... .......... 18% 75.9M 1s\n",
            " 10750K .......... .......... .......... .......... .......... 18% 34.2M 1s\n",
            " 10800K .......... .......... .......... .......... .......... 19% 68.7M 1s\n",
            " 10850K .......... .......... .......... .......... .......... 19% 73.8M 1s\n",
            " 10900K .......... .......... .......... .......... .......... 19% 76.7M 1s\n",
            " 10950K .......... .......... .......... .......... .......... 19% 84.8M 1s\n",
            " 11000K .......... .......... .......... .......... .......... 19% 96.3M 1s\n",
            " 11050K .......... .......... .......... .......... .......... 19% 83.1M 1s\n",
            " 11100K .......... .......... .......... .......... .......... 19% 72.8M 1s\n",
            " 11150K .......... .......... .......... .......... .......... 19% 69.3M 1s\n",
            " 11200K .......... .......... .......... .......... .......... 19% 75.2M 1s\n",
            " 11250K .......... .......... .......... .......... .......... 19% 35.2M 1s\n",
            " 11300K .......... .......... .......... .......... .......... 19% 70.8M 1s\n",
            " 11350K .......... .......... .......... .......... .......... 19% 69.1M 1s\n",
            " 11400K .......... .......... .......... .......... .......... 20% 78.0M 1s\n",
            " 11450K .......... .......... .......... .......... .......... 20% 73.3M 1s\n",
            " 11500K .......... .......... .......... .......... .......... 20% 87.0M 1s\n",
            " 11550K .......... .......... .......... .......... .......... 20% 65.0M 1s\n",
            " 11600K .......... .......... .......... .......... .......... 20% 72.1M 1s\n",
            " 11650K .......... .......... .......... .......... .......... 20% 61.3M 1s\n",
            " 11700K .......... .......... .......... .......... .......... 20% 72.5M 1s\n",
            " 11750K .......... .......... .......... .......... .......... 20% 33.0M 1s\n",
            " 11800K .......... .......... .......... .......... .......... 20% 77.0M 1s\n",
            " 11850K .......... .......... .......... .......... .......... 20% 69.1M 1s\n",
            " 11900K .......... .......... .......... .......... .......... 20% 65.5M 1s\n",
            " 11950K .......... .......... .......... .......... .......... 21% 63.0M 1s\n",
            " 12000K .......... .......... .......... .......... .......... 21% 83.5M 1s\n",
            " 12050K .......... .......... .......... .......... .......... 21% 74.6M 1s\n",
            " 12100K .......... .......... .......... .......... .......... 21% 76.2M 1s\n",
            " 12150K .......... .......... .......... .......... .......... 21% 65.5M 1s\n",
            " 12200K .......... .......... .......... .......... .......... 21% 82.7M 1s\n",
            " 12250K .......... .......... .......... .......... .......... 21% 90.8M 1s\n",
            " 12300K .......... .......... .......... .......... .......... 21% 39.8M 1s\n",
            " 12350K .......... .......... .......... .......... .......... 21% 70.9M 1s\n",
            " 12400K .......... .......... .......... .......... .......... 21% 66.4M 1s\n",
            " 12450K .......... .......... .......... .......... .......... 21% 76.3M 1s\n",
            " 12500K .......... .......... .......... .......... .......... 21% 68.3M 1s\n",
            " 12550K .......... .......... .......... .......... .......... 22% 70.9M 1s\n",
            " 12600K .......... .......... .......... .......... .......... 22% 89.5M 1s\n",
            " 12650K .......... .......... .......... .......... .......... 22% 76.2M 1s\n",
            " 12700K .......... .......... .......... .......... .......... 22% 73.3M 1s\n",
            " 12750K .......... .......... .......... .......... .......... 22% 71.7M 1s\n",
            " 12800K .......... .......... .......... .......... .......... 22% 34.6M 1s\n",
            " 12850K .......... .......... .......... .......... .......... 22% 86.3M 1s\n",
            " 12900K .......... .......... .......... .......... .......... 22% 81.1M 1s\n",
            " 12950K .......... .......... .......... .......... .......... 22% 70.9M 1s\n",
            " 13000K .......... .......... .......... .......... .......... 22% 83.7M 1s\n",
            " 13050K .......... .......... .......... .......... .......... 22% 65.5M 1s\n",
            " 13100K .......... .......... .......... .......... .......... 23% 64.8M 1s\n",
            " 13150K .......... .......... .......... .......... .......... 23% 60.3M 1s\n",
            " 13200K .......... .......... .......... .......... .......... 23% 75.1M 1s\n",
            " 13250K .......... .......... .......... .......... .......... 23% 63.7M 1s\n",
            " 13300K .......... .......... .......... .......... .......... 23% 34.8M 1s\n",
            " 13350K .......... .......... .......... .......... .......... 23% 70.5M 1s\n",
            " 13400K .......... .......... .......... .......... .......... 23% 74.0M 1s\n",
            " 13450K .......... .......... .......... .......... .......... 23% 34.1M 1s\n",
            " 13500K .......... .......... .......... .......... .......... 23% 33.5M 1s\n",
            " 13550K .......... .......... .......... .......... .......... 23% 54.4M 1s\n",
            " 13600K .......... .......... .......... .......... .......... 23% 79.8M 1s\n",
            " 13650K .......... .......... .......... .......... .......... 23% 65.6M 1s\n",
            " 13700K .......... .......... .......... .......... .......... 24% 64.3M 1s\n",
            " 13750K .......... .......... .......... .......... .......... 24% 39.1M 1s\n",
            " 13800K .......... .......... .......... .......... .......... 24% 25.8M 1s\n",
            " 13850K .......... .......... .......... .......... .......... 24% 45.8M 1s\n",
            " 13900K .......... .......... .......... .......... .......... 24% 20.0M 1s\n",
            " 13950K .......... .......... .......... .......... .......... 24% 56.5M 1s\n",
            " 14000K .......... .......... .......... .......... .......... 24% 67.4M 1s\n",
            " 14050K .......... .......... .......... .......... .......... 24% 74.6M 1s\n",
            " 14100K .......... .......... .......... .......... .......... 24% 66.9M 1s\n",
            " 14150K .......... .......... .......... .......... .......... 24% 67.4M 1s\n",
            " 14200K .......... .......... .......... .......... .......... 24% 80.7M 1s\n",
            " 14250K .......... .......... .......... .......... .......... 25% 71.3M 1s\n",
            " 14300K .......... .......... .......... .......... .......... 25% 73.9M 1s\n",
            " 14350K .......... .......... .......... .......... .......... 25% 32.5M 1s\n",
            " 14400K .......... .......... .......... .......... .......... 25% 62.6M 1s\n",
            " 14450K .......... .......... .......... .......... .......... 25% 73.4M 1s\n",
            " 14500K .......... .......... .......... .......... .......... 25% 72.2M 1s\n",
            " 14550K .......... .......... .......... .......... .......... 25% 65.7M 1s\n",
            " 14600K .......... .......... .......... .......... .......... 25% 59.5M 1s\n",
            " 14650K .......... .......... .......... .......... .......... 25% 75.1M 1s\n",
            " 14700K .......... .......... .......... .......... .......... 25% 74.7M 1s\n",
            " 14750K .......... .......... .......... .......... .......... 25% 59.0M 1s\n",
            " 14800K .......... .......... .......... .......... .......... 26% 60.7M 1s\n",
            " 14850K .......... .......... .......... .......... .......... 26% 28.7M 1s\n",
            " 14900K .......... .......... .......... .......... .......... 26% 65.9M 1s\n",
            " 14950K .......... .......... .......... .......... .......... 26% 65.1M 1s\n",
            " 15000K .......... .......... .......... .......... .......... 26% 73.5M 1s\n",
            " 15050K .......... .......... .......... .......... .......... 26% 60.2M 1s\n",
            " 15100K .......... .......... .......... .......... .......... 26% 77.3M 1s\n",
            " 15150K .......... .......... .......... .......... .......... 26% 59.9M 1s\n",
            " 15200K .......... .......... .......... .......... .......... 26% 75.4M 1s\n",
            " 15250K .......... .......... .......... .......... .......... 26% 75.5M 1s\n",
            " 15300K .......... .......... .......... .......... .......... 26% 61.8M 1s\n",
            " 15350K .......... .......... .......... .......... .......... 26% 33.1M 1s\n",
            " 15400K .......... .......... .......... .......... .......... 27% 74.4M 1s\n",
            " 15450K .......... .......... .......... .......... .......... 27% 72.0M 1s\n",
            " 15500K .......... .......... .......... .......... .......... 27% 66.8M 1s\n",
            " 15550K .......... .......... .......... .......... .......... 27% 59.0M 1s\n",
            " 15600K .......... .......... .......... .......... .......... 27% 76.1M 1s\n",
            " 15650K .......... .......... .......... .......... .......... 27% 68.5M 1s\n",
            " 15700K .......... .......... .......... .......... .......... 27% 71.6M 1s\n",
            " 15750K .......... .......... .......... .......... .......... 27% 66.9M 1s\n",
            " 15800K .......... .......... .......... .......... .......... 27% 64.1M 1s\n",
            " 15850K .......... .......... .......... .......... .......... 27% 35.0M 1s\n",
            " 15900K .......... .......... .......... .......... .......... 27% 37.3M 1s\n",
            " 15950K .......... .......... .......... .......... .......... 28% 53.5M 1s\n",
            " 16000K .......... .......... .......... .......... .......... 28% 80.4M 1s\n",
            " 16050K .......... .......... .......... .......... .......... 28% 77.5M 1s\n",
            " 16100K .......... .......... .......... .......... .......... 28% 56.9M 1s\n",
            " 16150K .......... .......... .......... .......... .......... 28% 60.3M 1s\n",
            " 16200K .......... .......... .......... .......... .......... 28% 71.4M 1s\n",
            " 16250K .......... .......... .......... .......... .......... 28% 63.4M 1s\n",
            " 16300K .......... .......... .......... .......... .......... 28% 70.1M 1s\n",
            " 16350K .......... .......... .......... .......... .......... 28% 37.7M 1s\n",
            " 16400K .......... .......... .......... .......... .......... 28% 66.5M 1s\n",
            " 16450K .......... .......... .......... .......... .......... 28% 64.8M 1s\n",
            " 16500K .......... .......... .......... .......... .......... 28% 67.9M 1s\n",
            " 16550K .......... .......... .......... .......... .......... 29% 56.7M 1s\n",
            " 16600K .......... .......... .......... .......... .......... 29% 64.4M 1s\n",
            " 16650K .......... .......... .......... .......... .......... 29% 62.4M 1s\n",
            " 16700K .......... .......... .......... .......... .......... 29% 66.0M 1s\n",
            " 16750K .......... .......... .......... .......... .......... 29% 63.4M 1s\n",
            " 16800K .......... .......... .......... .......... .......... 29% 73.4M 1s\n",
            " 16850K .......... .......... .......... .......... .......... 29% 75.1M 1s\n",
            " 16900K .......... .......... .......... .......... .......... 29% 32.5M 1s\n",
            " 16950K .......... .......... .......... .......... .......... 29% 51.8M 1s\n",
            " 17000K .......... .......... .......... .......... .......... 29% 61.8M 1s\n",
            " 17050K .......... .......... .......... .......... .......... 29% 66.7M 1s\n",
            " 17100K .......... .......... .......... .......... .......... 30% 69.9M 1s\n",
            " 17150K .......... .......... .......... .......... .......... 30% 56.0M 1s\n",
            " 17200K .......... .......... .......... .......... .......... 30% 76.7M 1s\n",
            " 17250K .......... .......... .......... .......... .......... 30% 71.3M 1s\n",
            " 17300K .......... .......... .......... .......... .......... 30% 76.3M 1s\n",
            " 17350K .......... .......... .......... .......... .......... 30% 67.0M 1s\n",
            " 17400K .......... .......... .......... .......... .......... 30% 37.1M 1s\n",
            " 17450K .......... .......... .......... .......... .......... 30% 65.0M 1s\n",
            " 17500K .......... .......... .......... .......... .......... 30% 63.2M 1s\n",
            " 17550K .......... .......... .......... .......... .......... 30% 57.8M 1s\n",
            " 17600K .......... .......... .......... .......... .......... 30% 67.8M 1s\n",
            " 17650K .......... .......... .......... .......... .......... 30% 68.4M 1s\n",
            " 17700K .......... .......... .......... .......... .......... 31% 84.6M 1s\n",
            " 17750K .......... .......... .......... .......... .......... 31% 69.9M 1s\n",
            " 17800K .......... .......... .......... .......... .......... 31% 76.2M 1s\n",
            " 17850K .......... .......... .......... .......... .......... 31% 74.5M 1s\n",
            " 17900K .......... .......... .......... .......... .......... 31% 34.3M 1s\n",
            " 17950K .......... .......... .......... .......... .......... 31% 53.4M 1s\n",
            " 18000K .......... .......... .......... .......... .......... 31% 65.2M 1s\n",
            " 18050K .......... .......... .......... .......... .......... 31% 70.7M 1s\n",
            " 18100K .......... .......... .......... .......... .......... 31% 55.6M 1s\n",
            " 18150K .......... .......... .......... .......... .......... 31% 49.4M 1s\n",
            " 18200K .......... .......... .......... .......... .......... 31% 76.6M 1s\n",
            " 18250K .......... .......... .......... .......... .......... 32% 80.9M 1s\n",
            " 18300K .......... .......... .......... .......... .......... 32% 78.1M 1s\n",
            " 18350K .......... .......... .......... .......... .......... 32% 64.0M 1s\n",
            " 18400K .......... .......... .......... .......... .......... 32% 33.6M 1s\n",
            " 18450K .......... .......... .......... .......... .......... 32% 45.0M 1s\n",
            " 18500K .......... .......... .......... .......... .......... 32% 69.5M 1s\n",
            " 18550K .......... .......... .......... .......... .......... 32% 60.6M 1s\n",
            " 18600K .......... .......... .......... .......... .......... 32% 79.0M 1s\n",
            " 18650K .......... .......... .......... .......... .......... 32% 69.6M 1s\n",
            " 18700K .......... .......... .......... .......... .......... 32% 75.7M 1s\n",
            " 18750K .......... .......... .......... .......... .......... 32% 60.5M 1s\n",
            " 18800K .......... .......... .......... .......... .......... 33% 56.6M 1s\n",
            " 18850K .......... .......... .......... .......... .......... 33% 64.1M 1s\n",
            " 18900K .......... .......... .......... .......... .......... 33% 61.9M 1s\n",
            " 18950K .......... .......... .......... .......... .......... 33% 29.8M 1s\n",
            " 19000K .......... .......... .......... .......... .......... 33% 62.9M 1s\n",
            " 19050K .......... .......... .......... .......... .......... 33% 77.4M 1s\n",
            " 19100K .......... .......... .......... .......... .......... 33% 69.4M 1s\n",
            " 19150K .......... .......... .......... .......... .......... 33% 64.1M 1s\n",
            " 19200K .......... .......... .......... .......... .......... 33% 88.6M 1s\n",
            " 19250K .......... .......... .......... .......... .......... 33% 85.0M 1s\n",
            " 19300K .......... .......... .......... .......... .......... 33% 74.0M 1s\n",
            " 19350K .......... .......... .......... .......... .......... 33% 62.1M 1s\n",
            " 19400K .......... .......... .......... .......... .......... 34% 68.3M 1s\n",
            " 19450K .......... .......... .......... .......... .......... 34% 32.4M 1s\n",
            " 19500K .......... .......... .......... .......... .......... 34% 75.6M 1s\n",
            " 19550K .......... .......... .......... .......... .......... 34% 62.6M 1s\n",
            " 19600K .......... .......... .......... .......... .......... 34% 70.1M 1s\n",
            " 19650K .......... .......... .......... .......... .......... 34% 89.9M 1s\n",
            " 19700K .......... .......... .......... .......... .......... 34% 84.8M 1s\n",
            " 19750K .......... .......... .......... .......... .......... 34% 72.9M 1s\n",
            " 19800K .......... .......... .......... .......... .......... 34% 68.8M 1s\n",
            " 19850K .......... .......... .......... .......... .......... 34% 64.1M 1s\n",
            " 19900K .......... .......... .......... .......... .......... 34% 69.8M 1s\n",
            " 19950K .......... .......... .......... .......... .......... 35% 29.0M 1s\n",
            " 20000K .......... .......... .......... .......... .......... 35% 73.7M 1s\n",
            " 20050K .......... .......... .......... .......... .......... 35% 82.6M 1s\n",
            " 20100K .......... .......... .......... .......... .......... 35% 68.9M 1s\n",
            " 20150K .......... .......... .......... .......... .......... 35% 64.0M 1s\n",
            " 20200K .......... .......... .......... .......... .......... 35% 78.2M 1s\n",
            " 20250K .......... .......... .......... .......... .......... 35% 71.8M 1s\n",
            " 20300K .......... .......... .......... .......... .......... 35% 69.9M 1s\n",
            " 20350K .......... .......... .......... .......... .......... 35% 56.0M 1s\n",
            " 20400K .......... .......... .......... .......... .......... 35% 73.8M 1s\n",
            " 20450K .......... .......... .......... .......... .......... 35% 36.2M 1s\n",
            " 20500K .......... .......... .......... .......... .......... 35% 55.3M 1s\n",
            " 20550K .......... .......... .......... .......... .......... 36% 66.5M 1s\n",
            " 20600K .......... .......... .......... .......... .......... 36% 66.5M 1s\n",
            " 20650K .......... .......... .......... .......... .......... 36% 72.0M 1s\n",
            " 20700K .......... .......... .......... .......... .......... 36% 85.5M 1s\n",
            " 20750K .......... .......... .......... .......... .......... 36% 56.2M 1s\n",
            " 20800K .......... .......... .......... .......... .......... 36% 78.9M 1s\n",
            " 20850K .......... .......... .......... .......... .......... 36% 84.8M 1s\n",
            " 20900K .......... .......... .......... .......... .......... 36% 73.1M 1s\n",
            " 20950K .......... .......... .......... .......... .......... 36% 58.2M 1s\n",
            " 21000K .......... .......... .......... .......... .......... 36% 36.8M 1s\n",
            " 21050K .......... .......... .......... .......... .......... 36% 79.7M 1s\n",
            " 21100K .......... .......... .......... .......... .......... 37% 81.9M 1s\n",
            " 21150K .......... .......... .......... .......... .......... 37% 67.0M 1s\n",
            " 21200K .......... .......... .......... .......... .......... 37% 81.7M 1s\n",
            " 21250K .......... .......... .......... .......... .......... 37% 81.7M 1s\n",
            " 21300K .......... .......... .......... .......... .......... 37% 63.5M 1s\n",
            " 21350K .......... .......... .......... .......... .......... 37% 68.4M 1s\n",
            " 21400K .......... .......... .......... .......... .......... 37% 73.9M 1s\n",
            " 21450K .......... .......... .......... .......... .......... 37% 68.7M 1s\n",
            " 21500K .......... .......... .......... .......... .......... 37% 33.8M 1s\n",
            " 21550K .......... .......... .......... .......... .......... 37% 65.1M 1s\n",
            " 21600K .......... .......... .......... .......... .......... 37% 75.0M 1s\n",
            " 21650K .......... .......... .......... .......... .......... 38% 76.3M 1s\n",
            " 21700K .......... .......... .......... .......... .......... 38% 77.5M 1s\n",
            " 21750K .......... .......... .......... .......... .......... 38% 68.8M 1s\n",
            " 21800K .......... .......... .......... .......... .......... 38% 64.1M 1s\n",
            " 21850K .......... .......... .......... .......... .......... 38% 62.4M 1s\n",
            " 21900K .......... .......... .......... .......... .......... 38% 74.3M 1s\n",
            " 21950K .......... .......... .......... .......... .......... 38% 63.0M 1s\n",
            " 22000K .......... .......... .......... .......... .......... 38% 35.1M 1s\n",
            " 22050K .......... .......... .......... .......... .......... 38% 67.6M 1s\n",
            " 22100K .......... .......... .......... .......... .......... 38% 66.7M 1s\n",
            " 22150K .......... .......... .......... .......... .......... 38% 76.0M 1s\n",
            " 22200K .......... .......... .......... .......... .......... 38% 81.0M 1s\n",
            " 22250K .......... .......... .......... .......... .......... 39% 75.9M 1s\n",
            " 22300K .......... .......... .......... .......... .......... 39% 53.1M 1s\n",
            " 22350K .......... .......... .......... .......... .......... 39% 52.6M 1s\n",
            " 22400K .......... .......... .......... .......... .......... 39% 68.7M 1s\n",
            " 22450K .......... .......... .......... .......... .......... 39% 75.1M 1s\n",
            " 22500K .......... .......... .......... .......... .......... 39% 35.6M 1s\n",
            " 22550K .......... .......... .......... .......... .......... 39% 57.5M 1s\n",
            " 22600K .......... .......... .......... .......... .......... 39% 61.2M 1s\n",
            " 22650K .......... .......... .......... .......... .......... 39% 65.5M 1s\n",
            " 22700K .......... .......... .......... .......... .......... 39% 81.0M 1s\n",
            " 22750K .......... .......... .......... .......... .......... 39% 62.1M 1s\n",
            " 22800K .......... .......... .......... .......... .......... 40% 72.0M 1s\n",
            " 22850K .......... .......... .......... .......... .......... 40% 81.0M 1s\n",
            " 22900K .......... .......... .......... .......... .......... 40% 66.1M 1s\n",
            " 22950K .......... .......... .......... .......... .......... 40% 70.0M 1s\n",
            " 23000K .......... .......... .......... .......... .......... 40% 73.5M 1s\n",
            " 23050K .......... .......... .......... .......... .......... 40% 35.5M 1s\n",
            " 23100K .......... .......... .......... .......... .......... 40% 70.9M 1s\n",
            " 23150K .......... .......... .......... .......... .......... 40% 54.1M 1s\n",
            " 23200K .......... .......... .......... .......... .......... 40% 74.4M 1s\n",
            " 23250K .......... .......... .......... .......... .......... 40% 79.2M 1s\n",
            " 23300K .......... .......... .......... .......... .......... 40% 59.7M 1s\n",
            " 23350K .......... .......... .......... .......... .......... 40% 58.2M 1s\n",
            " 23400K .......... .......... .......... .......... .......... 41% 67.3M 1s\n",
            " 23450K .......... .......... .......... .......... .......... 41% 79.9M 1s\n",
            " 23500K .......... .......... .......... .......... .......... 41% 63.9M 1s\n",
            " 23550K .......... .......... .......... .......... .......... 41% 33.6M 1s\n",
            " 23600K .......... .......... .......... .......... .......... 41% 79.0M 1s\n",
            " 23650K .......... .......... .......... .......... .......... 41% 76.7M 1s\n",
            " 23700K .......... .......... .......... .......... .......... 41% 83.8M 1s\n",
            " 23750K .......... .......... .......... .......... .......... 41% 71.3M 1s\n",
            " 23800K .......... .......... .......... .......... .......... 41% 67.3M 1s\n",
            " 23850K .......... .......... .......... .......... .......... 41% 68.6M 1s\n",
            " 23900K .......... .......... .......... .......... .......... 41% 74.5M 1s\n",
            " 23950K .......... .......... .......... .......... .......... 42% 64.0M 1s\n",
            " 24000K .......... .......... .......... .......... .......... 42% 70.1M 1s\n",
            " 24050K .......... .......... .......... .......... .......... 42% 34.3M 1s\n",
            " 24100K .......... .......... .......... .......... .......... 42% 78.5M 1s\n",
            " 24150K .......... .......... .......... .......... .......... 42% 57.3M 1s\n",
            " 24200K .......... .......... .......... .......... .......... 42% 83.9M 1s\n",
            " 24250K .......... .......... .......... .......... .......... 42% 80.6M 1s\n",
            " 24300K .......... .......... .......... .......... .......... 42% 70.7M 1s\n",
            " 24350K .......... .......... .......... .......... .......... 42% 63.3M 1s\n",
            " 24400K .......... .......... .......... .......... .......... 42% 76.2M 1s\n",
            " 24450K .......... .......... .......... .......... .......... 42% 78.1M 1s\n",
            " 24500K .......... .......... .......... .......... .......... 42% 91.0M 1s\n",
            " 24550K .......... .......... .......... .......... .......... 43% 33.1M 1s\n",
            " 24600K .......... .......... .......... .......... .......... 43% 85.0M 1s\n",
            " 24650K .......... .......... .......... .......... .......... 43% 65.7M 1s\n",
            " 24700K .......... .......... .......... .......... .......... 43% 75.0M 1s\n",
            " 24750K .......... .......... .......... .......... .......... 43% 71.3M 1s\n",
            " 24800K .......... .......... .......... .......... .......... 43% 76.6M 1s\n",
            " 24850K .......... .......... .......... .......... .......... 43% 81.0M 1s\n",
            " 24900K .......... .......... .......... .......... .......... 43% 66.3M 1s\n",
            " 24950K .......... .......... .......... .......... .......... 43% 63.4M 1s\n",
            " 25000K .......... .......... .......... .......... .......... 43% 77.0M 1s\n",
            " 25050K .......... .......... .......... .......... .......... 43% 59.9M 1s\n",
            " 25100K .......... .......... .......... .......... .......... 44% 34.3M 1s\n",
            " 25150K .......... .......... .......... .......... .......... 44% 60.5M 1s\n",
            " 25200K .......... .......... .......... .......... .......... 44% 81.7M 1s\n",
            " 25250K .......... .......... .......... .......... .......... 44% 80.8M 1s\n",
            " 25300K .......... .......... .......... .......... .......... 44% 74.1M 1s\n",
            " 25350K .......... .......... .......... .......... .......... 44% 62.8M 1s\n",
            " 25400K .......... .......... .......... .......... .......... 44% 72.0M 1s\n",
            " 25450K .......... .......... .......... .......... .......... 44% 70.8M 1s\n",
            " 25500K .......... .......... .......... .......... .......... 44% 81.5M 1s\n",
            " 25550K .......... .......... .......... .......... .......... 44% 54.7M 1s\n",
            " 25600K .......... .......... .......... .......... .......... 44% 36.4M 1s\n",
            " 25650K .......... .......... .......... .......... .......... 45% 70.5M 1s\n",
            " 25700K .......... .......... .......... .......... .......... 45% 64.8M 1s\n",
            " 25750K .......... .......... .......... .......... .......... 45% 71.0M 1s\n",
            " 25800K .......... .......... .......... .......... .......... 45% 63.6M 1s\n",
            " 25850K .......... .......... .......... .......... .......... 45% 84.0M 1s\n",
            " 25900K .......... .......... .......... .......... .......... 45% 70.5M 1s\n",
            " 25950K .......... .......... .......... .......... .......... 45% 70.2M 1s\n",
            " 26000K .......... .......... .......... .......... .......... 45% 86.4M 1s\n",
            " 26050K .......... .......... .......... .......... .......... 45% 67.5M 1s\n",
            " 26100K .......... .......... .......... .......... .......... 45% 36.3M 1s\n",
            " 26150K .......... .......... .......... .......... .......... 45% 64.5M 1s\n",
            " 26200K .......... .......... .......... .......... .......... 45% 60.5M 1s\n",
            " 26250K .......... .......... .......... .......... .......... 46% 80.9M 1s\n",
            " 26300K .......... .......... .......... .......... .......... 46% 73.9M 1s\n",
            " 26350K .......... .......... .......... .......... .......... 46% 60.6M 1s\n",
            " 26400K .......... .......... .......... .......... .......... 46% 85.3M 1s\n",
            " 26450K .......... .......... .......... .......... .......... 46% 78.4M 1s\n",
            " 26500K .......... .......... .......... .......... .......... 46% 79.3M 1s\n",
            " 26550K .......... .......... .......... .......... .......... 46% 56.4M 1s\n",
            " 26600K .......... .......... .......... .......... .......... 46% 34.3M 1s\n",
            " 26650K .......... .......... .......... .......... .......... 46% 64.3M 1s\n",
            " 26700K .......... .......... .......... .......... .......... 46% 73.6M 1s\n",
            " 26750K .......... .......... .......... .......... .......... 46% 80.3M 1s\n",
            " 26800K .......... .......... .......... .......... .......... 47% 89.1M 1s\n",
            " 26850K .......... .......... .......... .......... .......... 47% 63.9M 1s\n",
            " 26900K .......... .......... .......... .......... .......... 47% 68.4M 1s\n",
            " 26950K .......... .......... .......... .......... .......... 47% 73.4M 1s\n",
            " 27000K .......... .......... .......... .......... .......... 47% 90.6M 1s\n",
            " 27050K .......... .......... .......... .......... .......... 47% 78.1M 1s\n",
            " 27100K .......... .......... .......... .......... .......... 47% 90.9M 1s\n",
            " 27150K .......... .......... .......... .......... .......... 47% 28.9M 1s\n",
            " 27200K .......... .......... .......... .......... .......... 47% 63.5M 1s\n",
            " 27250K .......... .......... .......... .......... .......... 47% 83.7M 1s\n",
            " 27300K .......... .......... .......... .......... .......... 47% 74.9M 1s\n",
            " 27350K .......... .......... .......... .......... .......... 47% 54.7M 1s\n",
            " 27400K .......... .......... .......... .......... .......... 48% 67.7M 1s\n",
            " 27450K .......... .......... .......... .......... .......... 48% 84.4M 1s\n",
            " 27500K .......... .......... .......... .......... .......... 48% 82.6M 1s\n",
            " 27550K .......... .......... .......... .......... .......... 48% 55.0M 1s\n",
            " 27600K .......... .......... .......... .......... .......... 48% 65.0M 1s\n",
            " 27650K .......... .......... .......... .......... .......... 48% 34.3M 1s\n",
            " 27700K .......... .......... .......... .......... .......... 48% 65.9M 1s\n",
            " 27750K .......... .......... .......... .......... .......... 48% 69.9M 1s\n",
            " 27800K .......... .......... .......... .......... .......... 48% 65.6M 1s\n",
            " 27850K .......... .......... .......... .......... .......... 48% 64.8M 1s\n",
            " 27900K .......... .......... .......... .......... .......... 48% 71.8M 1s\n",
            " 27950K .......... .......... .......... .......... .......... 49% 60.2M 0s\n",
            " 28000K .......... .......... .......... .......... .......... 49% 77.0M 0s\n",
            " 28050K .......... .......... .......... .......... .......... 49% 73.5M 0s\n",
            " 28100K .......... .......... .......... .......... .......... 49% 74.9M 0s\n",
            " 28150K .......... .......... .......... .......... .......... 49% 33.6M 0s\n",
            " 28200K .......... .......... .......... .......... .......... 49% 71.4M 0s\n",
            " 28250K .......... .......... .......... .......... .......... 49% 76.7M 0s\n",
            " 28300K .......... .......... .......... .......... .......... 49% 74.0M 0s\n",
            " 28350K .......... .......... .......... .......... .......... 49% 64.2M 0s\n",
            " 28400K .......... .......... .......... .......... .......... 49% 84.9M 0s\n",
            " 28450K .......... .......... .......... .......... .......... 49% 86.6M 0s\n",
            " 28500K .......... .......... .......... .......... .......... 50% 68.7M 0s\n",
            " 28550K .......... .......... .......... .......... .......... 50% 62.3M 0s\n",
            " 28600K .......... .......... .......... .......... .......... 50% 77.2M 0s\n",
            " 28650K .......... .......... .......... .......... .......... 50% 37.7M 0s\n",
            " 28700K .......... .......... .......... .......... .......... 50% 79.5M 0s\n",
            " 28750K .......... .......... .......... .......... .......... 50% 55.4M 0s\n",
            " 28800K .......... .......... .......... .......... .......... 50% 67.0M 0s\n",
            " 28850K .......... .......... .......... .......... .......... 50% 69.4M 0s\n",
            " 28900K .......... .......... .......... .......... .......... 50% 65.9M 0s\n",
            " 28950K .......... .......... .......... .......... .......... 50% 53.3M 0s\n",
            " 29000K .......... .......... .......... .......... .......... 50% 69.1M 0s\n",
            " 29050K .......... .......... .......... .......... .......... 50% 60.2M 0s\n",
            " 29100K .......... .......... .......... .......... .......... 51% 58.2M 0s\n",
            " 29150K .......... .......... .......... .......... .......... 51% 24.7M 0s\n",
            " 29200K .......... .......... .......... .......... .......... 51% 54.7M 0s\n",
            " 29250K .......... .......... .......... .......... .......... 51% 68.9M 0s\n",
            " 29300K .......... .......... .......... .......... .......... 51% 72.2M 0s\n",
            " 29350K .......... .......... .......... .......... .......... 51% 59.1M 0s\n",
            " 29400K .......... .......... .......... .......... .......... 51% 72.2M 0s\n",
            " 29450K .......... .......... .......... .......... .......... 51% 81.0M 0s\n",
            " 29500K .......... .......... .......... .......... .......... 51% 65.2M 0s\n",
            " 29550K .......... .......... .......... .......... .......... 51% 60.3M 0s\n",
            " 29600K .......... .......... .......... .......... .......... 51% 67.6M 0s\n",
            " 29650K .......... .......... .......... .......... .......... 52% 75.5M 0s\n",
            " 29700K .......... .......... .......... .......... .......... 52% 31.1M 0s\n",
            " 29750K .......... .......... .......... .......... .......... 52% 58.0M 0s\n",
            " 29800K .......... .......... .......... .......... .......... 52% 67.2M 0s\n",
            " 29850K .......... .......... .......... .......... .......... 52% 53.0M 0s\n",
            " 29900K .......... .......... .......... .......... .......... 52% 78.3M 0s\n",
            " 29950K .......... .......... .......... .......... .......... 52% 64.3M 0s\n",
            " 30000K .......... .......... .......... .......... .......... 52% 86.0M 0s\n",
            " 30050K .......... .......... .......... .......... .......... 52% 68.0M 0s\n",
            " 30100K .......... .......... .......... .......... .......... 52% 76.3M 0s\n",
            " 30150K .......... .......... .......... .......... .......... 52% 63.1M 0s\n",
            " 30200K .......... .......... .......... .......... .......... 52% 34.2M 0s\n",
            " 30250K .......... .......... .......... .......... .......... 53% 68.2M 0s\n",
            " 30300K .......... .......... .......... .......... .......... 53% 52.7M 0s\n",
            " 30350K .......... .......... .......... .......... .......... 53% 61.9M 0s\n",
            " 30400K .......... .......... .......... .......... .......... 53% 72.5M 0s\n",
            " 30450K .......... .......... .......... .......... .......... 53% 72.0M 0s\n",
            " 30500K .......... .......... .......... .......... .......... 53% 78.1M 0s\n",
            " 30550K .......... .......... .......... .......... .......... 53% 65.3M 0s\n",
            " 30600K .......... .......... .......... .......... .......... 53% 83.5M 0s\n",
            " 30650K .......... .......... .......... .......... .......... 53% 57.6M 0s\n",
            " 30700K .......... .......... .......... .......... .......... 53% 31.9M 0s\n",
            " 30750K .......... .......... .......... .......... .......... 53% 52.4M 0s\n",
            " 30800K .......... .......... .......... .......... .......... 54% 67.1M 0s\n",
            " 30850K .......... .......... .......... .......... .......... 54% 80.9M 0s\n",
            " 30900K .......... .......... .......... .......... .......... 54% 78.7M 0s\n",
            " 30950K .......... .......... .......... .......... .......... 54% 71.6M 0s\n",
            " 31000K .......... .......... .......... .......... .......... 54% 76.8M 0s\n",
            " 31050K .......... .......... .......... .......... .......... 54% 80.4M 0s\n",
            " 31100K .......... .......... .......... .......... .......... 54% 80.3M 0s\n",
            " 31150K .......... .......... .......... .......... .......... 54% 49.2M 0s\n",
            " 31200K .......... .......... .......... .......... .......... 54% 35.7M 0s\n",
            " 31250K .......... .......... .......... .......... .......... 54% 61.7M 0s\n",
            " 31300K .......... .......... .......... .......... .......... 54% 64.5M 0s\n",
            " 31350K .......... .......... .......... .......... .......... 54% 60.2M 0s\n",
            " 31400K .......... .......... .......... .......... .......... 55% 74.4M 0s\n",
            " 31450K .......... .......... .......... .......... .......... 55% 84.9M 0s\n",
            " 31500K .......... .......... .......... .......... .......... 55% 64.0M 0s\n",
            " 31550K .......... .......... .......... .......... .......... 55% 65.0M 0s\n",
            " 31600K .......... .......... .......... .......... .......... 55% 66.9M 0s\n",
            " 31650K .......... .......... .......... .......... .......... 55% 70.7M 0s\n",
            " 31700K .......... .......... .......... .......... .......... 55% 65.2M 0s\n",
            " 31750K .......... .......... .......... .......... .......... 55% 29.0M 0s\n",
            " 31800K .......... .......... .......... .......... .......... 55% 60.1M 0s\n",
            " 31850K .......... .......... .......... .......... .......... 55% 75.4M 0s\n",
            " 31900K .......... .......... .......... .......... .......... 55% 84.9M 0s\n",
            " 31950K .......... .......... .......... .......... .......... 56% 71.4M 0s\n",
            " 32000K .......... .......... .......... .......... .......... 56% 82.3M 0s\n",
            " 32050K .......... .......... .......... .......... .......... 56% 83.5M 0s\n",
            " 32100K .......... .......... .......... .......... .......... 56% 70.7M 0s\n",
            " 32150K .......... .......... .......... .......... .......... 56% 54.9M 0s\n",
            " 32200K .......... .......... .......... .......... .......... 56% 62.4M 0s\n",
            " 32250K .......... .......... .......... .......... .......... 56% 32.2M 0s\n",
            " 32300K .......... .......... .......... .......... .......... 56% 28.6M 0s\n",
            " 32350K .......... .......... .......... .......... .......... 56% 69.4M 0s\n",
            " 32400K .......... .......... .......... .......... .......... 56% 94.7M 0s\n",
            " 32450K .......... .......... .......... .......... .......... 56% 85.8M 0s\n",
            " 32500K .......... .......... .......... .......... .......... 57% 78.5M 0s\n",
            " 32550K .......... .......... .......... .......... .......... 57% 55.0M 0s\n",
            " 32600K .......... .......... .......... .......... .......... 57% 68.2M 0s\n",
            " 32650K .......... .......... .......... .......... .......... 57% 73.2M 0s\n",
            " 32700K .......... .......... .......... .......... .......... 57% 72.9M 0s\n",
            " 32750K .......... .......... .......... .......... .......... 57% 30.6M 0s\n",
            " 32800K .......... .......... .......... .......... .......... 57% 82.1M 0s\n",
            " 32850K .......... .......... .......... .......... .......... 57% 83.3M 0s\n",
            " 32900K .......... .......... .......... .......... .......... 57% 83.2M 0s\n",
            " 32950K .......... .......... .......... .......... .......... 57% 72.1M 0s\n",
            " 33000K .......... .......... .......... .......... .......... 57% 69.2M 0s\n",
            " 33050K .......... .......... .......... .......... .......... 57% 71.0M 0s\n",
            " 33100K .......... .......... .......... .......... .......... 58% 59.1M 0s\n",
            " 33150K .......... .......... .......... .......... .......... 58% 63.1M 0s\n",
            " 33200K .......... .......... .......... .......... .......... 58% 78.1M 0s\n",
            " 33250K .......... .......... .......... .......... .......... 58% 34.0M 0s\n",
            " 33300K .......... .......... .......... .......... .......... 58% 78.3M 0s\n",
            " 33350K .......... .......... .......... .......... .......... 58% 72.1M 0s\n",
            " 33400K .......... .......... .......... .......... .......... 58% 76.0M 0s\n",
            " 33450K .......... .......... .......... .......... .......... 58% 79.5M 0s\n",
            " 33500K .......... .......... .......... .......... .......... 58% 50.7M 0s\n",
            " 33550K .......... .......... .......... .......... .......... 58% 53.3M 0s\n",
            " 33600K .......... .......... .......... .......... .......... 58% 64.3M 0s\n",
            " 33650K .......... .......... .......... .......... .......... 59% 73.5M 0s\n",
            " 33700K .......... .......... .......... .......... .......... 59% 77.2M 0s\n",
            " 33750K .......... .......... .......... .......... .......... 59% 55.5M 0s\n",
            " 33800K .......... .......... .......... .......... .......... 59% 35.9M 0s\n",
            " 33850K .......... .......... .......... .......... .......... 59% 73.1M 0s\n",
            " 33900K .......... .......... .......... .......... .......... 59% 75.7M 0s\n",
            " 33950K .......... .......... .......... .......... .......... 59% 54.9M 0s\n",
            " 34000K .......... .......... .......... .......... .......... 59% 66.3M 0s\n",
            " 34050K .......... .......... .......... .......... .......... 59% 63.0M 0s\n",
            " 34100K .......... .......... .......... .......... .......... 59% 66.2M 0s\n",
            " 34150K .......... .......... .......... .......... .......... 59% 60.3M 0s\n",
            " 34200K .......... .......... .......... .......... .......... 59% 72.0M 0s\n",
            " 34250K .......... .......... .......... .......... .......... 60% 67.1M 0s\n",
            " 34300K .......... .......... .......... .......... .......... 60% 35.8M 0s\n",
            " 34350K .......... .......... .......... .......... .......... 60% 63.7M 0s\n",
            " 34400K .......... .......... .......... .......... .......... 60% 71.1M 0s\n",
            " 34450K .......... .......... .......... .......... .......... 60% 76.9M 0s\n",
            " 34500K .......... .......... .......... .......... .......... 60% 78.6M 0s\n",
            " 34550K .......... .......... .......... .......... .......... 60% 64.8M 0s\n",
            " 34600K .......... .......... .......... .......... .......... 60% 78.7M 0s\n",
            " 34650K .......... .......... .......... .......... .......... 60% 79.8M 0s\n",
            " 34700K .......... .......... .......... .......... .......... 60% 52.2M 0s\n",
            " 34750K .......... .......... .......... .......... .......... 60% 51.6M 0s\n",
            " 34800K .......... .......... .......... .......... .......... 61% 33.7M 0s\n",
            " 34850K .......... .......... .......... .......... .......... 61% 60.4M 0s\n",
            " 34900K .......... .......... .......... .......... .......... 61% 85.7M 0s\n",
            " 34950K .......... .......... .......... .......... .......... 61% 77.5M 0s\n",
            " 35000K .......... .......... .......... .......... .......... 61% 82.8M 0s\n",
            " 35050K .......... .......... .......... .......... .......... 61% 81.4M 0s\n",
            " 35100K .......... .......... .......... .......... .......... 61% 84.3M 0s\n",
            " 35150K .......... .......... .......... .......... .......... 61% 60.1M 0s\n",
            " 35200K .......... .......... .......... .......... .......... 61% 69.6M 0s\n",
            " 35250K .......... .......... .......... .......... .......... 61% 69.2M 0s\n",
            " 35300K .......... .......... .......... .......... .......... 61% 34.4M 0s\n",
            " 35350K .......... .......... .......... .......... .......... 61% 68.4M 0s\n",
            " 35400K .......... .......... .......... .......... .......... 62% 69.9M 0s\n",
            " 35450K .......... .......... .......... .......... .......... 62% 63.5M 0s\n",
            " 35500K .......... .......... .......... .......... .......... 62% 83.3M 0s\n",
            " 35550K .......... .......... .......... .......... .......... 62% 72.4M 0s\n",
            " 35600K .......... .......... .......... .......... .......... 62% 80.4M 0s\n",
            " 35650K .......... .......... .......... .......... .......... 62% 61.2M 0s\n",
            " 35700K .......... .......... .......... .......... .......... 62% 83.0M 0s\n",
            " 35750K .......... .......... .......... .......... .......... 62% 51.6M 0s\n",
            " 35800K .......... .......... .......... .......... .......... 62% 64.0M 0s\n",
            " 35850K .......... .......... .......... .......... .......... 62% 38.9M 0s\n",
            " 35900K .......... .......... .......... .......... .......... 62%  107M 0s\n",
            " 35950K .......... .......... .......... .......... .......... 63% 88.7M 0s\n",
            " 36000K .......... .......... .......... .......... .......... 63%  114M 0s\n",
            " 36050K .......... .......... .......... .......... .......... 63%  114M 0s\n",
            " 36100K .......... .......... .......... .......... .......... 63% 96.3M 0s\n",
            " 36150K .......... .......... .......... .......... .......... 63% 91.3M 0s\n",
            " 36200K .......... .......... .......... .......... .......... 63% 99.3M 0s\n",
            " 36250K .......... .......... .......... .......... .......... 63%  100M 0s\n",
            " 36300K .......... .......... .......... .......... .......... 63%  113M 0s\n",
            " 36350K .......... .......... .......... .......... .......... 63% 36.2M 0s\n",
            " 36400K .......... .......... .......... .......... .......... 63%  105M 0s\n",
            " 36450K .......... .......... .......... .......... .......... 63% 88.5M 0s\n",
            " 36500K .......... .......... .......... .......... .......... 64% 99.7M 0s\n",
            " 36550K .......... .......... .......... .......... .......... 64% 89.6M 0s\n",
            " 36600K .......... .......... .......... .......... .......... 64%  102M 0s\n",
            " 36650K .......... .......... .......... .......... .......... 64%  100M 0s\n",
            " 36700K .......... .......... .......... .......... .......... 64%  104M 0s\n",
            " 36750K .......... .......... .......... .......... .......... 64% 80.9M 0s\n",
            " 36800K .......... .......... .......... .......... .......... 64% 89.9M 0s\n",
            " 36850K .......... .......... .......... .......... .......... 64% 36.1M 0s\n",
            " 36900K .......... .......... .......... .......... .......... 64% 87.6M 0s\n",
            " 36950K .......... .......... .......... .......... .......... 64% 72.1M 0s\n",
            " 37000K .......... .......... .......... .......... .......... 64% 86.5M 0s\n",
            " 37050K .......... .......... .......... .......... .......... 64% 92.9M 0s\n",
            " 37100K .......... .......... .......... .......... .......... 65% 88.8M 0s\n",
            " 37150K .......... .......... .......... .......... .......... 65% 32.0M 0s\n",
            " 37200K .......... .......... .......... .......... .......... 65% 71.0M 0s\n",
            " 37250K .......... .......... .......... .......... .......... 65% 76.8M 0s\n",
            " 37300K .......... .......... .......... .......... .......... 65% 90.8M 0s\n",
            " 37350K .......... .......... .......... .......... .......... 65% 33.3M 0s\n",
            " 37400K .......... .......... .......... .......... .......... 65% 82.0M 0s\n",
            " 37450K .......... .......... .......... .......... .......... 65% 70.4M 0s\n",
            " 37500K .......... .......... .......... .......... .......... 65% 83.0M 0s\n",
            " 37550K .......... .......... .......... .......... .......... 65% 61.2M 0s\n",
            " 37600K .......... .......... .......... .......... .......... 65% 86.6M 0s\n",
            " 37650K .......... .......... .......... .......... .......... 66% 84.3M 0s\n",
            " 37700K .......... .......... .......... .......... .......... 66% 70.3M 0s\n",
            " 37750K .......... .......... .......... .......... .......... 66% 57.8M 0s\n",
            " 37800K .......... .......... .......... .......... .......... 66% 68.2M 0s\n",
            " 37850K .......... .......... .......... .......... .......... 66% 74.9M 0s\n",
            " 37900K .......... .......... .......... .......... .......... 66% 35.4M 0s\n",
            " 37950K .......... .......... .......... .......... .......... 66% 56.9M 0s\n",
            " 38000K .......... .......... .......... .......... .......... 66% 77.5M 0s\n",
            " 38050K .......... .......... .......... .......... .......... 66% 83.3M 0s\n",
            " 38100K .......... .......... .......... .......... .......... 66% 70.4M 0s\n",
            " 38150K .......... .......... .......... .......... .......... 66% 67.7M 0s\n",
            " 38200K .......... .......... .......... .......... .......... 66% 63.7M 0s\n",
            " 38250K .......... .......... .......... .......... .......... 67% 48.2M 0s\n",
            " 38300K .......... .......... .......... .......... .......... 67% 76.9M 0s\n",
            " 38350K .......... .......... .......... .......... .......... 67% 53.1M 0s\n",
            " 38400K .......... .......... .......... .......... .......... 67% 32.2M 0s\n",
            " 38450K .......... .......... .......... .......... .......... 67% 66.5M 0s\n",
            " 38500K .......... .......... .......... .......... .......... 67% 70.5M 0s\n",
            " 38550K .......... .......... .......... .......... .......... 67% 63.3M 0s\n",
            " 38600K .......... .......... .......... .......... .......... 67% 69.2M 0s\n",
            " 38650K .......... .......... .......... .......... .......... 67% 78.3M 0s\n",
            " 38700K .......... .......... .......... .......... .......... 67% 85.9M 0s\n",
            " 38750K .......... .......... .......... .......... .......... 67% 71.8M 0s\n",
            " 38800K .......... .......... .......... .......... .......... 68% 64.0M 0s\n",
            " 38850K .......... .......... .......... .......... .......... 68% 51.0M 0s\n",
            " 38900K .......... .......... .......... .......... .......... 68% 36.0M 0s\n",
            " 38950K .......... .......... .......... .......... .......... 68% 50.1M 0s\n",
            " 39000K .......... .......... .......... .......... .......... 68% 68.6M 0s\n",
            " 39050K .......... .......... .......... .......... .......... 68% 74.7M 0s\n",
            " 39100K .......... .......... .......... .......... .......... 68% 75.6M 0s\n",
            " 39150K .......... .......... .......... .......... .......... 68% 45.4M 0s\n",
            " 39200K .......... .......... .......... .......... .......... 68% 47.4M 0s\n",
            " 39250K .......... .......... .......... .......... .......... 68% 76.1M 0s\n",
            " 39300K .......... .......... .......... .......... .......... 68% 74.0M 0s\n",
            " 39350K .......... .......... .......... .......... .......... 69% 65.3M 0s\n",
            " 39400K .......... .......... .......... .......... .......... 69% 36.1M 0s\n",
            " 39450K .......... .......... .......... .......... .......... 69% 75.7M 0s\n",
            " 39500K .......... .......... .......... .......... .......... 69% 73.5M 0s\n",
            " 39550K .......... .......... .......... .......... .......... 69% 65.1M 0s\n",
            " 39600K .......... .......... .......... .......... .......... 69% 94.6M 0s\n",
            " 39650K .......... .......... .......... .......... .......... 69% 87.9M 0s\n",
            " 39700K .......... .......... .......... .......... .......... 69% 71.3M 0s\n",
            " 39750K .......... .......... .......... .......... .......... 69% 63.4M 0s\n",
            " 39800K .......... .......... .......... .......... .......... 69% 85.2M 0s\n",
            " 39850K .......... .......... .......... .......... .......... 69% 87.3M 0s\n",
            " 39900K .......... .......... .......... .......... .......... 69% 82.2M 0s\n",
            " 39950K .......... .......... .......... .......... .......... 70% 34.1M 0s\n",
            " 40000K .......... .......... .......... .......... .......... 70% 62.6M 0s\n",
            " 40050K .......... .......... .......... .......... .......... 70% 74.7M 0s\n",
            " 40100K .......... .......... .......... .......... .......... 70% 80.0M 0s\n",
            " 40150K .......... .......... .......... .......... .......... 70% 67.1M 0s\n",
            " 40200K .......... .......... .......... .......... .......... 70% 63.8M 0s\n",
            " 40250K .......... .......... .......... .......... .......... 70% 79.4M 0s\n",
            " 40300K .......... .......... .......... .......... .......... 70% 61.9M 0s\n",
            " 40350K .......... .......... .......... .......... .......... 70% 54.3M 0s\n",
            " 40400K .......... .......... .......... .......... .......... 70% 76.1M 0s\n",
            " 40450K .......... .......... .......... .......... .......... 70% 31.0M 0s\n",
            " 40500K .......... .......... .......... .......... .......... 71% 61.1M 0s\n",
            " 40550K .......... .......... .......... .......... .......... 71% 58.6M 0s\n",
            " 40600K .......... .......... .......... .......... .......... 71% 69.7M 0s\n",
            " 40650K .......... .......... .......... .......... .......... 71% 62.8M 0s\n",
            " 40700K .......... .......... .......... .......... .......... 71% 67.2M 0s\n",
            " 40750K .......... .......... .......... .......... .......... 71% 59.6M 0s\n",
            " 40800K .......... .......... .......... .......... .......... 71% 77.1M 0s\n",
            " 40850K .......... .......... .......... .......... .......... 71% 75.0M 0s\n",
            " 40900K .......... .......... .......... .......... .......... 71% 73.9M 0s\n",
            " 40950K .......... .......... .......... .......... .......... 71% 31.7M 0s\n",
            " 41000K .......... .......... .......... .......... .......... 71% 63.5M 0s\n",
            " 41050K .......... .......... .......... .......... .......... 71% 62.9M 0s\n",
            " 41100K .......... .......... .......... .......... .......... 72% 67.8M 0s\n",
            " 41150K .......... .......... .......... .......... .......... 72% 57.5M 0s\n",
            " 41200K .......... .......... .......... .......... .......... 72% 70.9M 0s\n",
            " 41250K .......... .......... .......... .......... .......... 72% 66.2M 0s\n",
            " 41300K .......... .......... .......... .......... .......... 72% 69.9M 0s\n",
            " 41350K .......... .......... .......... .......... .......... 72% 73.1M 0s\n",
            " 41400K .......... .......... .......... .......... .......... 72% 77.4M 0s\n",
            " 41450K .......... .......... .......... .......... .......... 72% 38.0M 0s\n",
            " 41500K .......... .......... .......... .......... .......... 72% 80.2M 0s\n",
            " 41550K .......... .......... .......... .......... .......... 72% 67.1M 0s\n",
            " 41600K .......... .......... .......... .......... .......... 72% 79.9M 0s\n",
            " 41650K .......... .......... .......... .......... .......... 73% 80.8M 0s\n",
            " 41700K .......... .......... .......... .......... .......... 73% 84.5M 0s\n",
            " 41750K .......... .......... .......... .......... .......... 73% 54.1M 0s\n",
            " 41800K .......... .......... .......... .......... .......... 73% 64.7M 0s\n",
            " 41850K .......... .......... .......... .......... .......... 73% 70.2M 0s\n",
            " 41900K .......... .......... .......... .......... .......... 73% 69.9M 0s\n",
            " 41950K .......... .......... .......... .......... .......... 73% 29.8M 0s\n",
            " 42000K .......... .......... .......... .......... .......... 73% 77.1M 0s\n",
            " 42050K .......... .......... .......... .......... .......... 73% 85.2M 0s\n",
            " 42100K .......... .......... .......... .......... .......... 73% 78.7M 0s\n",
            " 42150K .......... .......... .......... .......... .......... 73% 72.2M 0s\n",
            " 42200K .......... .......... .......... .......... .......... 73% 81.3M 0s\n",
            " 42250K .......... .......... .......... .......... .......... 74% 80.5M 0s\n",
            " 42300K .......... .......... .......... .......... .......... 74% 81.3M 0s\n",
            " 42350K .......... .......... .......... .......... .......... 74% 57.8M 0s\n",
            " 42400K .......... .......... .......... .......... .......... 74% 66.0M 0s\n",
            " 42450K .......... .......... .......... .......... .......... 74% 75.5M 0s\n",
            " 42500K .......... .......... .......... .......... .......... 74% 37.1M 0s\n",
            " 42550K .......... .......... .......... .......... .......... 74% 64.1M 0s\n",
            " 42600K .......... .......... .......... .......... .......... 74% 78.2M 0s\n",
            " 42650K .......... .......... .......... .......... .......... 74% 83.1M 0s\n",
            " 42700K .......... .......... .......... .......... .......... 74% 59.7M 0s\n",
            " 42750K .......... .......... .......... .......... .......... 74% 50.9M 0s\n",
            " 42800K .......... .......... .......... .......... .......... 75% 65.7M 0s\n",
            " 42850K .......... .......... .......... .......... .......... 75% 65.8M 0s\n",
            " 42900K .......... .......... .......... .......... .......... 75% 63.6M 0s\n",
            " 42950K .......... .......... .......... .......... .......... 75% 55.2M 0s\n",
            " 43000K .......... .......... .......... .......... .......... 75% 32.4M 0s\n",
            " 43050K .......... .......... .......... .......... .......... 75% 75.4M 0s\n",
            " 43100K .......... .......... .......... .......... .......... 75% 71.2M 0s\n",
            " 43150K .......... .......... .......... .......... .......... 75% 46.2M 0s\n",
            " 43200K .......... .......... .......... .......... .......... 75% 60.6M 0s\n",
            " 43250K .......... .......... .......... .......... .......... 75% 55.4M 0s\n",
            " 43300K .......... .......... .......... .......... .......... 75% 69.4M 0s\n",
            " 43350K .......... .......... .......... .......... .......... 76% 54.7M 0s\n",
            " 43400K .......... .......... .......... .......... .......... 76% 64.2M 0s\n",
            " 43450K .......... .......... .......... .......... .......... 76% 66.8M 0s\n",
            " 43500K .......... .......... .......... .......... .......... 76% 30.7M 0s\n",
            " 43550K .......... .......... .......... .......... .......... 76% 58.9M 0s\n",
            " 43600K .......... .......... .......... .......... .......... 76% 66.0M 0s\n",
            " 43650K .......... .......... .......... .......... .......... 76% 79.0M 0s\n",
            " 43700K .......... .......... .......... .......... .......... 76% 76.2M 0s\n",
            " 43750K .......... .......... .......... .......... .......... 76% 74.2M 0s\n",
            " 43800K .......... .......... .......... .......... .......... 76% 79.5M 0s\n",
            " 43850K .......... .......... .......... .......... .......... 76% 78.6M 0s\n",
            " 43900K .......... .......... .......... .......... .......... 76% 61.7M 0s\n",
            " 43950K .......... .......... .......... .......... .......... 77% 51.2M 0s\n",
            " 44000K .......... .......... .......... .......... .......... 77% 33.1M 0s\n",
            " 44050K .......... .......... .......... .......... .......... 77% 62.1M 0s\n",
            " 44100K .......... .......... .......... .......... .......... 77% 71.8M 0s\n",
            " 44150K .......... .......... .......... .......... .......... 77% 66.7M 0s\n",
            " 44200K .......... .......... .......... .......... .......... 77% 77.6M 0s\n",
            " 44250K .......... .......... .......... .......... .......... 77% 89.4M 0s\n",
            " 44300K .......... .......... .......... .......... .......... 77% 89.0M 0s\n",
            " 44350K .......... .......... .......... .......... .......... 77% 80.9M 0s\n",
            " 44400K .......... .......... .......... .......... .......... 77% 63.9M 0s\n",
            " 44450K .......... .......... .......... .......... .......... 77% 64.2M 0s\n",
            " 44500K .......... .......... .......... .......... .......... 78% 65.5M 0s\n",
            " 44550K .......... .......... .......... .......... .......... 78% 26.3M 0s\n",
            " 44600K .......... .......... .......... .......... .......... 78% 68.6M 0s\n",
            " 44650K .......... .......... .......... .......... .......... 78% 83.6M 0s\n",
            " 44700K .......... .......... .......... .......... .......... 78% 91.0M 0s\n",
            " 44750K .......... .......... .......... .......... .......... 78% 72.3M 0s\n",
            " 44800K .......... .......... .......... .......... .......... 78% 88.5M 0s\n",
            " 44850K .......... .......... .......... .......... .......... 78% 81.0M 0s\n",
            " 44900K .......... .......... .......... .......... .......... 78% 51.8M 0s\n",
            " 44950K .......... .......... .......... .......... .......... 78% 60.0M 0s\n",
            " 45000K .......... .......... .......... .......... .......... 78% 72.3M 0s\n",
            " 45050K .......... .......... .......... .......... .......... 78% 32.1M 0s\n",
            " 45100K .......... .......... .......... .......... .......... 79% 81.6M 0s\n",
            " 45150K .......... .......... .......... .......... .......... 79% 69.9M 0s\n",
            " 45200K .......... .......... .......... .......... .......... 79% 84.0M 0s\n",
            " 45250K .......... .......... .......... .......... .......... 79% 81.5M 0s\n",
            " 45300K .......... .......... .......... .......... .......... 79% 78.5M 0s\n",
            " 45350K .......... .......... .......... .......... .......... 79% 74.1M 0s\n",
            " 45400K .......... .......... .......... .......... .......... 79% 58.8M 0s\n",
            " 45450K .......... .......... .......... .......... .......... 79% 67.4M 0s\n",
            " 45500K .......... .......... .......... .......... .......... 79% 74.4M 0s\n",
            " 45550K .......... .......... .......... .......... .......... 79% 31.0M 0s\n",
            " 45600K .......... .......... .......... .......... .......... 79% 79.6M 0s\n",
            " 45650K .......... .......... .......... .......... .......... 80% 81.0M 0s\n",
            " 45700K .......... .......... .......... .......... .......... 80% 78.3M 0s\n",
            " 45750K .......... .......... .......... .......... .......... 80% 72.0M 0s\n",
            " 45800K .......... .......... .......... .......... .......... 80% 74.0M 0s\n",
            " 45850K .......... .......... .......... .......... .......... 80% 81.7M 0s\n",
            " 45900K .......... .......... .......... .......... .......... 80% 63.0M 0s\n",
            " 45950K .......... .......... .......... .......... .......... 80% 58.5M 0s\n",
            " 46000K .......... .......... .......... .......... .......... 80% 66.0M 0s\n",
            " 46050K .......... .......... .......... .......... .......... 80% 28.9M 0s\n",
            " 46100K .......... .......... .......... .......... .......... 80% 70.7M 0s\n",
            " 46150K .......... .......... .......... .......... .......... 80% 69.6M 0s\n",
            " 46200K .......... .......... .......... .......... .......... 81% 76.9M 0s\n",
            " 46250K .......... .......... .......... .......... .......... 81% 75.7M 0s\n",
            " 46300K .......... .......... .......... .......... .......... 81% 78.6M 0s\n",
            " 46350K .......... .......... .......... .......... .......... 81% 71.0M 0s\n",
            " 46400K .......... .......... .......... .......... .......... 81% 69.6M 0s\n",
            " 46450K .......... .......... .......... .......... .......... 81% 59.4M 0s\n",
            " 46500K .......... .......... .......... .......... .......... 81% 77.4M 0s\n",
            " 46550K .......... .......... .......... .......... .......... 81% 65.5M 0s\n",
            " 46600K .......... .......... .......... .......... .......... 81% 34.2M 0s\n",
            " 46650K .......... .......... .......... .......... .......... 81% 82.3M 0s\n",
            " 46700K .......... .......... .......... .......... .......... 81% 67.3M 0s\n",
            " 46750K .......... .......... .......... .......... .......... 81% 24.0M 0s\n",
            " 46800K .......... .......... .......... .......... .......... 82% 68.4M 0s\n",
            " 46850K .......... .......... .......... .......... .......... 82% 66.7M 0s\n",
            " 46900K .......... .......... .......... .......... .......... 82% 70.1M 0s\n",
            " 46950K .......... .......... .......... .......... .......... 82% 63.9M 0s\n",
            " 47000K .......... .......... .......... .......... .......... 82% 83.0M 0s\n",
            " 47050K .......... .......... .......... .......... .......... 82% 82.3M 0s\n",
            " 47100K .......... .......... .......... .......... .......... 82% 40.3M 0s\n",
            " 47150K .......... .......... .......... .......... .......... 82% 49.0M 0s\n",
            " 47200K .......... .......... .......... .......... .......... 82% 77.4M 0s\n",
            " 47250K .......... .......... .......... .......... .......... 82% 80.8M 0s\n",
            " 47300K .......... .......... .......... .......... .......... 82% 61.6M 0s\n",
            " 47350K .......... .......... .......... .......... .......... 83% 65.0M 0s\n",
            " 47400K .......... .......... .......... .......... .......... 83% 74.9M 0s\n",
            " 47450K .......... .......... .......... .......... .......... 83% 77.6M 0s\n",
            " 47500K .......... .......... .......... .......... .......... 83% 82.4M 0s\n",
            " 47550K .......... .......... .......... .......... .......... 83% 67.9M 0s\n",
            " 47600K .......... .......... .......... .......... .......... 83% 30.3M 0s\n",
            " 47650K .......... .......... .......... .......... .......... 83% 71.6M 0s\n",
            " 47700K .......... .......... .......... .......... .......... 83% 79.4M 0s\n",
            " 47750K .......... .......... .......... .......... .......... 83% 70.3M 0s\n",
            " 47800K .......... .......... .......... .......... .......... 83% 59.4M 0s\n",
            " 47850K .......... .......... .......... .......... .......... 83% 81.0M 0s\n",
            " 47900K .......... .......... .......... .......... .......... 83% 82.6M 0s\n",
            " 47950K .......... .......... .......... .......... .......... 84% 72.1M 0s\n",
            " 48000K .......... .......... .......... .......... .......... 84% 82.2M 0s\n",
            " 48050K .......... .......... .......... .......... .......... 84% 80.6M 0s\n",
            " 48100K .......... .......... .......... .......... .......... 84% 37.0M 0s\n",
            " 48150K .......... .......... .......... .......... .......... 84% 47.1M 0s\n",
            " 48200K .......... .......... .......... .......... .......... 84% 67.5M 0s\n",
            " 48250K .......... .......... .......... .......... .......... 84% 82.0M 0s\n",
            " 48300K .......... .......... .......... .......... .......... 84% 82.9M 0s\n",
            " 48350K .......... .......... .......... .......... .......... 84% 67.2M 0s\n",
            " 48400K .......... .......... .......... .......... .......... 84% 66.6M 0s\n",
            " 48450K .......... .......... .......... .......... .......... 84% 54.3M 0s\n",
            " 48500K .......... .......... .......... .......... .......... 85% 80.1M 0s\n",
            " 48550K .......... .......... .......... .......... .......... 85% 75.0M 0s\n",
            " 48600K .......... .......... .......... .......... .......... 85% 65.1M 0s\n",
            " 48650K .......... .......... .......... .......... .......... 85% 31.6M 0s\n",
            " 48700K .......... .......... .......... .......... .......... 85% 81.4M 0s\n",
            " 48750K .......... .......... .......... .......... .......... 85% 74.2M 0s\n",
            " 48800K .......... .......... .......... .......... .......... 85% 86.1M 0s\n",
            " 48850K .......... .......... .......... .......... .......... 85% 83.4M 0s\n",
            " 48900K .......... .......... .......... .......... .......... 85% 79.6M 0s\n",
            " 48950K .......... .......... .......... .......... .......... 85% 53.1M 0s\n",
            " 49000K .......... .......... .......... .......... .......... 85% 74.7M 0s\n",
            " 49050K .......... .......... .......... .......... .......... 85% 85.7M 0s\n",
            " 49100K .......... .......... .......... .......... .......... 86% 73.3M 0s\n",
            " 49150K .......... .......... .......... .......... .......... 86% 27.9M 0s\n",
            " 49200K .......... .......... .......... .......... .......... 86% 74.5M 0s\n",
            " 49250K .......... .......... .......... .......... .......... 86% 84.9M 0s\n",
            " 49300K .......... .......... .......... .......... .......... 86% 85.0M 0s\n",
            " 49350K .......... .......... .......... .......... .......... 86% 71.8M 0s\n",
            " 49400K .......... .......... .......... .......... .......... 86% 87.1M 0s\n",
            " 49450K .......... .......... .......... .......... .......... 86% 55.8M 0s\n",
            " 49500K .......... .......... .......... .......... .......... 86% 84.9M 0s\n",
            " 49550K .......... .......... .......... .......... .......... 86% 70.8M 0s\n",
            " 49600K .......... .......... .......... .......... .......... 86% 84.9M 0s\n",
            " 49650K .......... .......... .......... .......... .......... 87% 39.3M 0s\n",
            " 49700K .......... .......... .......... .......... .......... 87% 79.6M 0s\n",
            " 49750K .......... .......... .......... .......... .......... 87% 66.7M 0s\n",
            " 49800K .......... .......... .......... .......... .......... 87% 78.2M 0s\n",
            " 49850K .......... .......... .......... .......... .......... 87% 82.5M 0s\n",
            " 49900K .......... .......... .......... .......... .......... 87% 87.6M 0s\n",
            " 49950K .......... .......... .......... .......... .......... 87% 60.4M 0s\n",
            " 50000K .......... .......... .......... .......... .......... 87% 67.7M 0s\n",
            " 50050K .......... .......... .......... .......... .......... 87% 66.6M 0s\n",
            " 50100K .......... .......... .......... .......... .......... 87% 77.8M 0s\n",
            " 50150K .......... .......... .......... .......... .......... 87% 34.2M 0s\n",
            " 50200K .......... .......... .......... .......... .......... 88% 72.7M 0s\n",
            " 50250K .......... .......... .......... .......... .......... 88% 82.8M 0s\n",
            " 50300K .......... .......... .......... .......... .......... 88% 79.3M 0s\n",
            " 50350K .......... .......... .......... .......... .......... 88% 66.8M 0s\n",
            " 50400K .......... .......... .......... .......... .......... 88% 80.9M 0s\n",
            " 50450K .......... .......... .......... .......... .......... 88% 79.9M 0s\n",
            " 50500K .......... .......... .......... .......... .......... 88% 73.4M 0s\n",
            " 50550K .......... .......... .......... .......... .......... 88% 64.8M 0s\n",
            " 50600K .......... .......... .......... .......... .......... 88% 65.4M 0s\n",
            " 50650K .......... .......... .......... .......... .......... 88% 70.9M 0s\n",
            " 50700K .......... .......... .......... .......... .......... 88% 35.4M 0s\n",
            " 50750K .......... .......... .......... .......... .......... 88% 69.2M 0s\n",
            " 50800K .......... .......... .......... .......... .......... 89% 78.8M 0s\n",
            " 50850K .......... .......... .......... .......... .......... 89% 73.8M 0s\n",
            " 50900K .......... .......... .......... .......... .......... 89% 80.4M 0s\n",
            " 50950K .......... .......... .......... .......... .......... 89% 64.1M 0s\n",
            " 51000K .......... .......... .......... .......... .......... 89% 71.4M 0s\n",
            " 51050K .......... .......... .......... .......... .......... 89% 51.5M 0s\n",
            " 51100K .......... .......... .......... .......... .......... 89% 66.9M 0s\n",
            " 51150K .......... .......... .......... .......... .......... 89% 62.1M 0s\n",
            " 51200K .......... .......... .......... .......... .......... 89% 36.7M 0s\n",
            " 51250K .......... .......... .......... .......... .......... 89% 78.6M 0s\n",
            " 51300K .......... .......... .......... .......... .......... 89% 80.9M 0s\n",
            " 51350K .......... .......... .......... .......... .......... 90% 76.5M 0s\n",
            " 51400K .......... .......... .......... .......... .......... 90% 85.9M 0s\n",
            " 51450K .......... .......... .......... .......... .......... 90% 79.7M 0s\n",
            " 51500K .......... .......... .......... .......... .......... 90% 86.5M 0s\n",
            " 51550K .......... .......... .......... .......... .......... 90% 64.5M 0s\n",
            " 51600K .......... .......... .......... .......... .......... 90% 74.9M 0s\n",
            " 51650K .......... .......... .......... .......... .......... 90% 71.4M 0s\n",
            " 51700K .......... .......... .......... .......... .......... 90% 30.8M 0s\n",
            " 51750K .......... .......... .......... .......... .......... 90% 52.1M 0s\n",
            " 51800K .......... .......... .......... .......... .......... 90% 75.0M 0s\n",
            " 51850K .......... .......... .......... .......... .......... 90% 76.9M 0s\n",
            " 51900K .......... .......... .......... .......... .......... 90% 82.7M 0s\n",
            " 51950K .......... .......... .......... .......... .......... 91% 73.2M 0s\n",
            " 52000K .......... .......... .......... .......... .......... 91% 86.6M 0s\n",
            " 52050K .......... .......... .......... .......... .......... 91% 71.8M 0s\n",
            " 52100K .......... .......... .......... .......... .......... 91% 48.1M 0s\n",
            " 52150K .......... .......... .......... .......... .......... 91% 56.4M 0s\n",
            " 52200K .......... .......... .......... .......... .......... 91% 32.2M 0s\n",
            " 52250K .......... .......... .......... .......... .......... 91% 76.5M 0s\n",
            " 52300K .......... .......... .......... .......... .......... 91% 58.9M 0s\n",
            " 52350K .......... .......... .......... .......... .......... 91% 61.7M 0s\n",
            " 52400K .......... .......... .......... .......... .......... 91% 69.1M 0s\n",
            " 52450K .......... .......... .......... .......... .......... 91% 85.7M 0s\n",
            " 52500K .......... .......... .......... .......... .......... 92% 70.5M 0s\n",
            " 52550K .......... .......... .......... .......... .......... 92% 63.4M 0s\n",
            " 52600K .......... .......... .......... .......... .......... 92% 77.3M 0s\n",
            " 52650K .......... .......... .......... .......... .......... 92% 64.6M 0s\n",
            " 52700K .......... .......... .......... .......... .......... 92% 82.8M 0s\n",
            " 52750K .......... .......... .......... .......... .......... 92% 35.1M 0s\n",
            " 52800K .......... .......... .......... .......... .......... 92% 60.5M 0s\n",
            " 52850K .......... .......... .......... .......... .......... 92% 71.7M 0s\n",
            " 52900K .......... .......... .......... .......... .......... 92% 84.2M 0s\n",
            " 52950K .......... .......... .......... .......... .......... 92% 68.8M 0s\n",
            " 53000K .......... .......... .......... .......... .......... 92% 86.2M 0s\n",
            " 53050K .......... .......... .......... .......... .......... 92% 91.3M 0s\n",
            " 53100K .......... .......... .......... .......... .......... 93% 90.7M 0s\n",
            " 53150K .......... .......... .......... .......... .......... 93% 70.9M 0s\n",
            " 53200K .......... .......... .......... .......... .......... 93% 86.3M 0s\n",
            " 53250K .......... .......... .......... .......... .......... 93% 36.9M 0s\n",
            " 53300K .......... .......... .......... .......... .......... 93% 69.7M 0s\n",
            " 53350K .......... .......... .......... .......... .......... 93% 68.7M 0s\n",
            " 53400K .......... .......... .......... .......... .......... 93% 80.6M 0s\n",
            " 53450K .......... .......... .......... .......... .......... 93% 84.3M 0s\n",
            " 53500K .......... .......... .......... .......... .......... 93% 89.1M 0s\n",
            " 53550K .......... .......... .......... .......... .......... 93% 69.2M 0s\n",
            " 53600K .......... .......... .......... .......... .......... 93% 76.3M 0s\n",
            " 53650K .......... .......... .......... .......... .......... 94% 84.2M 0s\n",
            " 53700K .......... .......... .......... .......... .......... 94% 83.7M 0s\n",
            " 53750K .......... .......... .......... .......... .......... 94% 36.5M 0s\n",
            " 53800K .......... .......... .......... .......... .......... 94% 79.8M 0s\n",
            " 53850K .......... .......... .......... .......... .......... 94% 73.4M 0s\n",
            " 53900K .......... .......... .......... .......... .......... 94% 78.4M 0s\n",
            " 53950K .......... .......... .......... .......... .......... 94% 61.9M 0s\n",
            " 54000K .......... .......... .......... .......... .......... 94% 75.2M 0s\n",
            " 54050K .......... .......... .......... .......... .......... 94% 78.9M 0s\n",
            " 54100K .......... .......... .......... .......... .......... 94% 83.4M 0s\n",
            " 54150K .......... .......... .......... .......... .......... 94% 75.7M 0s\n",
            " 54200K .......... .......... .......... .......... .......... 95% 86.5M 0s\n",
            " 54250K .......... .......... .......... .......... .......... 95% 40.1M 0s\n",
            " 54300K .......... .......... .......... .......... .......... 95% 80.6M 0s\n",
            " 54350K .......... .......... .......... .......... .......... 95% 64.5M 0s\n",
            " 54400K .......... .......... .......... .......... .......... 95% 85.0M 0s\n",
            " 54450K .......... .......... .......... .......... .......... 95% 79.8M 0s\n",
            " 54500K .......... .......... .......... .......... .......... 95% 84.7M 0s\n",
            " 54550K .......... .......... .......... .......... .......... 95% 65.5M 0s\n",
            " 54600K .......... .......... .......... .......... .......... 95% 76.5M 0s\n",
            " 54650K .......... .......... .......... .......... .......... 95% 76.6M 0s\n",
            " 54700K .......... .......... .......... .......... .......... 95% 78.4M 0s\n",
            " 54750K .......... .......... .......... .......... .......... 95% 37.3M 0s\n",
            " 54800K .......... .......... .......... .......... .......... 96% 75.2M 0s\n",
            " 54850K .......... .......... .......... .......... .......... 96% 83.5M 0s\n",
            " 54900K .......... .......... .......... .......... .......... 96% 86.9M 0s\n",
            " 54950K .......... .......... .......... .......... .......... 96% 70.8M 0s\n",
            " 55000K .......... .......... .......... .......... .......... 96% 87.1M 0s\n",
            " 55050K .......... .......... .......... .......... .......... 96% 84.3M 0s\n",
            " 55100K .......... .......... .......... .......... .......... 96% 79.9M 0s\n",
            " 55150K .......... .......... .......... .......... .......... 96% 69.8M 0s\n",
            " 55200K .......... .......... .......... .......... .......... 96% 86.0M 0s\n",
            " 55250K .......... .......... .......... .......... .......... 96% 86.5M 0s\n",
            " 55300K .......... .......... .......... .......... .......... 96% 38.4M 0s\n",
            " 55350K .......... .......... .......... .......... .......... 97% 71.2M 0s\n",
            " 55400K .......... .......... .......... .......... .......... 97% 77.4M 0s\n",
            " 55450K .......... .......... .......... .......... .......... 97% 81.6M 0s\n",
            " 55500K .......... .......... .......... .......... .......... 97% 88.5M 0s\n",
            " 55550K .......... .......... .......... .......... .......... 97% 74.0M 0s\n",
            " 55600K .......... .......... .......... .......... .......... 97% 89.5M 0s\n",
            " 55650K .......... .......... .......... .......... .......... 97% 72.7M 0s\n",
            " 55700K .......... .......... .......... .......... .......... 97% 85.0M 0s\n",
            " 55750K .......... .......... .......... .......... .......... 97% 74.1M 0s\n",
            " 55800K .......... .......... .......... .......... .......... 97% 37.6M 0s\n",
            " 55850K .......... .......... .......... .......... .......... 97% 77.0M 0s\n",
            " 55900K .......... .......... .......... .......... .......... 97% 80.2M 0s\n",
            " 55950K .......... .......... .......... .......... .......... 98% 63.7M 0s\n",
            " 56000K .......... .......... .......... .......... .......... 98% 82.5M 0s\n",
            " 56050K .......... .......... .......... .......... .......... 98% 81.2M 0s\n",
            " 56100K .......... .......... .......... .......... .......... 98% 85.7M 0s\n",
            " 56150K .......... .......... .......... .......... .......... 98% 73.0M 0s\n",
            " 56200K .......... .......... .......... .......... .......... 98% 78.1M 0s\n",
            " 56250K .......... .......... .......... .......... .......... 98% 69.2M 0s\n",
            " 56300K .......... .......... .......... .......... .......... 98% 39.0M 0s\n",
            " 56350K .......... .......... .......... .......... .......... 98% 63.7M 0s\n",
            " 56400K .......... .......... .......... .......... .......... 98% 79.6M 0s\n",
            " 56450K .......... .......... .......... .......... .......... 98% 82.8M 0s\n",
            " 56500K .......... .......... .......... .......... .......... 99% 87.4M 0s\n",
            " 56550K .......... .......... .......... .......... .......... 99% 71.0M 0s\n",
            " 56600K .......... .......... .......... .......... .......... 99% 83.0M 0s\n",
            " 56650K .......... .......... .......... .......... .......... 99% 90.0M 0s\n",
            " 56700K .......... .......... .......... .......... .......... 99% 80.5M 0s\n",
            " 56750K .......... .......... .......... .......... .......... 99% 68.5M 0s\n",
            " 56800K .......... .......... .......... .......... .......... 99% 40.4M 0s\n",
            " 56850K .......... .......... .......... .......... .......... 99% 75.7M 0s\n",
            " 56900K .......... .......... .......... .......... .......... 99% 79.8M 0s\n",
            " 56950K .......... .......... .......... .......... .......... 99% 74.9M 0s\n",
            " 57000K .......... .......... .......... .......... .......... 99% 79.8M 0s\n",
            " 57050K .......... .......... .......... .......... ........  100% 79.3M=0.9s\n",
            "\n",
            "2020-10-14 23:08:24 (60.3 MB/s) - ‘Miniconda3-4.5.4-Linux-x86_64.sh.1’ saved [58468498/58468498]\n",
            "\n",
            "Python 3.6.5 :: Anaconda, Inc.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62ELnj47TC1M",
        "outputId": "3430b07f-3079-4e2d-90cc-77d0b71951af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%bash\n",
        "conda install --channel defaults conda python=3.6 --yes\n",
        "conda update --channel defaults --all --yes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs: \n",
            "    - conda\n",
            "    - python=3.6\n",
            "\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "    ca-certificates: 2018.03.07-0            --> 2020.10.14-0           \n",
            "    certifi:         2018.4.16-py36_0        --> 2020.6.20-py36_0       \n",
            "    cffi:            1.11.5-py36h9745a5d_0   --> 1.14.3-py36he30daa8_0  \n",
            "    conda:           4.5.4-py36_0            --> 4.8.5-py36_0           \n",
            "    cryptography:    2.2.2-py36h14c3975_0    --> 3.1.1-py36h1ba5d50_0   \n",
            "    libedit:         3.1.20170329-h6b74fdf_2 --> 3.1.20191231-h14c3975_1\n",
            "    libffi:          3.2.1-hd88cf55_4        --> 3.3-he6710b0_2         \n",
            "    libgcc-ng:       7.2.0-hdf63c60_3        --> 9.1.0-hdf63c60_0       \n",
            "    libstdcxx-ng:    7.2.0-hdf63c60_3        --> 9.1.0-hdf63c60_0       \n",
            "    ncurses:         6.1-hf484d3e_0          --> 6.2-he6710b0_1         \n",
            "    openssl:         1.0.2o-h20670df_0       --> 1.1.1h-h7b6447c_0      \n",
            "    python:          3.6.5-hc3d631a_2        --> 3.6.12-hcff3b4d_2      \n",
            "    readline:        7.0-ha6073c6_4          --> 8.0-h7b6447c_0         \n",
            "    sqlite:          3.23.1-he433501_0       --> 3.33.0-h62c20be_0      \n",
            "    tk:              8.6.7-hc745277_3        --> 8.6.10-hbc83047_0      \n",
            "    xz:              5.2.4-h14c3975_4        --> 5.2.5-h7b6447c_0       \n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "\n",
            "The following packages will be REMOVED:\n",
            "\n",
            "  asn1crypto-0.24.0-py36_0\n",
            "  conda-env-2.6.0-h36134e3_1\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  chardet                              3.0.4-py36h0f667ec_1 --> 3.0.4-py36_1003\n",
            "  idna               pkgs/main/linux-64::idna-2.6-py36h82f~ --> pkgs/main/noarch::idna-2.10-py_0\n",
            "  pip                                         10.0.1-py36_0 --> 20.2.3-py36_0\n",
            "  pycparser          pkgs/main/linux-64::pycparser-2.18-py~ --> pkgs/main/noarch::pycparser-2.20-py_2\n",
            "  pyopenssl          pkgs/main/linux-64::pyopenssl-18.0.0-~ --> pkgs/main/noarch::pyopenssl-19.1.0-py_1\n",
            "  pysocks                                      1.6.8-py36_0 --> 1.7.1-py36_0\n",
            "  requests           pkgs/main/linux-64::requests-2.18.4-p~ --> pkgs/main/noarch::requests-2.24.0-py_0\n",
            "  ruamel_yaml                        0.15.37-py36h14c3975_2 --> 0.15.87-py36h7b6447c_1\n",
            "  setuptools                                  39.2.0-py36_0 --> 50.3.0-py36hb0f4dca_1\n",
            "  six                pkgs/main/linux-64::six-1.11.0-py36h3~ --> pkgs/main/noarch::six-1.15.0-py_0\n",
            "  urllib3            pkgs/main/linux-64::urllib3-1.22-py36~ --> pkgs/main/noarch::urllib3-1.25.10-py_0\n",
            "  wheel              pkgs/main/linux-64::wheel-0.31.1-py36~ --> pkgs/main/noarch::wheel-0.35.1-py_0\n",
            "  yaml                                     0.1.7-had09818_2 --> 0.2.5-h7b6447c_0\n",
            "  zlib                                    1.2.11-ha838bed_2 --> 1.2.11-h7b6447c_3\n",
            "\n",
            "The following packages will be DOWNGRADED:\n",
            "\n",
            "  pycosat                              0.6.3-py36h0a5515d_0 --> 0.6.3-py36h7b6447c_0\n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZASbTiofT1Ct"
      },
      "source": [
        "import sys\n",
        "sys.path\n",
        "\n",
        "_ = (sys.path\n",
        "        .append(\"/usr/local/lib/python3.6/site-packages\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OonIO5igVssX",
        "outputId": "48c3c0ca-e9ce-4d5c-d730-c0238a677107",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        }
      },
      "source": [
        "!conda create -n layoutlm python=3.6 --yes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/layoutlm\n",
            "\n",
            "  added / updated specs:\n",
            "    - python=3.6\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2020.10.14-0\n",
            "  certifi            pkgs/main/linux-64::certifi-2020.6.20-py36_0\n",
            "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.33.1-h53a641e_7\n",
            "  libedit            pkgs/main/linux-64::libedit-3.1.20191231-h14c3975_1\n",
            "  libffi             pkgs/main/linux-64::libffi-3.3-he6710b0_2\n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-9.1.0-hdf63c60_0\n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-9.1.0-hdf63c60_0\n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.2-he6710b0_1\n",
            "  openssl            pkgs/main/linux-64::openssl-1.1.1h-h7b6447c_0\n",
            "  pip                pkgs/main/linux-64::pip-20.2.3-py36_0\n",
            "  python             pkgs/main/linux-64::python-3.6.12-hcff3b4d_2\n",
            "  readline           pkgs/main/linux-64::readline-8.0-h7b6447c_0\n",
            "  setuptools         pkgs/main/linux-64::setuptools-50.3.0-py36hb0f4dca_1\n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.33.0-h62c20be_0\n",
            "  tk                 pkgs/main/linux-64::tk-8.6.10-hbc83047_0\n",
            "  wheel              pkgs/main/noarch::wheel-0.35.1-py_0\n",
            "  xz                 pkgs/main/linux-64::xz-5.2.5-h7b6447c_0\n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.11-h7b6447c_3\n",
            "\n",
            "\n",
            "Preparing transaction: | \b\b/ \b\b- \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate layoutlm\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCQNBlykWCI9"
      },
      "source": [
        "%%bash\n",
        "source /usr/local/etc/profile.d/conda.sh\n",
        "conda activate layoutlm\n",
        "#conda install pytorch==1.6.0 cudatoolkit=10.1 -c pytorch --yes # código do github do artigo. Entretanto, apresenta incompatibilidades. \n",
        "#Abaixo o torch==1.5.0 é instalado  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7L3e42HW6W4",
        "outputId": "45be4518-0053-40b8-a1b4-45606953c814",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%cd \"/content/drive/My Drive/Colab Notebooks/unilm/layoutlm/\"\n",
        "!git clone https://github.com/NVIDIA/apex"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/unilm/layoutlm\n",
            "fatal: destination path 'apex' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Rdw_G_bvN4I",
        "outputId": "7d3e867e-c455-4732-a780-f4fcde794a66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "!pip install torch==1.5.0+cu101 torchvision==0.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.5.0+cu101 in /usr/local/lib/python3.6/site-packages (1.5.0+cu101)\n",
            "Requirement already satisfied: torchvision==0.6.0+cu101 in /usr/local/lib/python3.6/site-packages (0.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (from torch==1.5.0+cu101) (1.19.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/site-packages (from torch==1.5.0+cu101) (0.18.2)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/site-packages (from torchvision==0.6.0+cu101) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woV4btxGW-6u",
        "outputId": "70b28691-2565-452d-cdf6-e46945fac0f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "%cd \"/content/drive/My Drive/Colab Notebooks/unilm/layoutlm/apex\"\n",
        "!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/unilm/layoutlm/apex\n",
            "/usr/local/lib/python3.6/site-packages/pip/_internal/commands/install.py:235: UserWarning: Disabling all use of wheels due to the use of --build-option / --global-option / --install-option.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "Using pip 20.2.3 from /usr/local/lib/python3.6/site-packages/pip (python 3.6)\n",
            "Non-user install because site-packages writeable\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-3ldnsz_k\n",
            "Created temporary directory: /tmp/pip-req-tracker-x4gz8hvj\n",
            "Initialized build tracking at /tmp/pip-req-tracker-x4gz8hvj\n",
            "Created build tracker: /tmp/pip-req-tracker-x4gz8hvj\n",
            "Entered build tracker: /tmp/pip-req-tracker-x4gz8hvj\n",
            "Created temporary directory: /tmp/pip-install-9qac79w3\n",
            "Processing /content/drive/My Drive/Colab Notebooks/unilm/layoutlm/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-rvbm_rmv\n",
            "  Added file:///content/drive/My%20Drive/Colab%20Notebooks/unilm/layoutlm/apex to build tracker '/tmp/pip-req-tracker-x4gz8hvj'\n",
            "    Running setup.py (path:/tmp/pip-req-build-rvbm_rmv/setup.py) egg_info for package from file:///content/drive/My%20Drive/Colab%20Notebooks/unilm/layoutlm/apex\n",
            "    Created temporary directory: /tmp/pip-pip-egg-info-8__lnu5h\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.5.0+cu101\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-pip-egg-info-8__lnu5h/apex.egg-info\n",
            "    writing /tmp/pip-pip-egg-info-8__lnu5h/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-pip-egg-info-8__lnu5h/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-pip-egg-info-8__lnu5h/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-8__lnu5h/apex.egg-info/SOURCES.txt'\n",
            "    reading manifest file '/tmp/pip-pip-egg-info-8__lnu5h/apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-8__lnu5h/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-rvbm_rmv/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-rvbm_rmv has version 0.1, which satisfies requirement apex==0.1 from file:///content/drive/My%20Drive/Colab%20Notebooks/unilm/layoutlm/apex\n",
            "  Removed apex==0.1 from file:///content/drive/My%20Drive/Colab%20Notebooks/unilm/layoutlm/apex from build tracker '/tmp/pip-req-tracker-x4gz8hvj'\n",
            "Skipping wheel build for apex, due to binaries being disabled for it.\n",
            "Installing collected packages: apex\n",
            "  Attempting uninstall: apex\n",
            "    Found existing installation: apex 0.1\n",
            "    Uninstalling apex-0.1:\n",
            "      Created temporary directory: /usr/local/lib/python3.6/site-packages/~pex-0.1.dist-info\n",
            "      Removing file or directory /usr/local/lib/python3.6/site-packages/apex-0.1.dist-info/\n",
            "      Created temporary directory: /usr/local/lib/python3.6/site-packages/~pex\n",
            "      Removing file or directory /usr/local/lib/python3.6/site-packages/apex/\n",
            "      Successfully uninstalled apex-0.1\n",
            "  Created temporary directory: /tmp/pip-record-ff4fstv4\n",
            "    Running command /usr/local/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-rvbm_rmv/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-rvbm_rmv/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-ff4fstv4/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.6m/apex\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.5.0+cu101\n",
            "\n",
            "\n",
            "    /tmp/pip-req-build-rvbm_rmv/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "\n",
            "    Compiling cuda extensions with\n",
            "    nvcc: NVIDIA (R) Cuda compiler driver\n",
            "    Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "    Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "    Cuda compilation tools, release 10.1, V10.1.243\n",
            "    from /usr/local/cuda/bin\n",
            "\n",
            "    running install\n",
            "    running build\n",
            "    running build_py\n",
            "    creating build\n",
            "    creating build/lib.linux-x86_64-3.6\n",
            "    creating build/lib.linux-x86_64-3.6/apex\n",
            "    copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\n",
            "    creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    creating build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    creating build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    creating build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    creating build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    creating build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "    copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib\n",
            "    copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib\n",
            "    creating build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    running build_ext\n",
            "    /usr/local/lib/python3.6/site-packages/torch/utils/cpp_extension.py:304: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "      warnings.warn(msg.format('we could not find ninja.'))\n",
            "    building 'apex_C' extension\n",
            "    creating build/temp.linux-x86_64-3.6\n",
            "    creating build/temp.linux-x86_64-3.6/csrc\n",
            "    gcc -pthread -B /usr/local/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
            "    In file included from csrc/flatten_unflatten.cpp:2:0:\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/utils/tensor_flatten.h: In member function ‘at::DeprecatedTypeProperties& torch::utils::TensorGroup::type()’:\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/utils/tensor_flatten.h:36:28: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         return tensors[0].type();\n",
            "                                ^\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/flatten_unflatten.cpp:1:\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    g++ -pthread -shared -B /usr/local/compiler_compat -L/usr/local/lib -Wl,-rpath=/usr/local/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -L/usr/local/lib/python3.6/site-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'amp_C' extension\n",
            "    gcc -pthread -B /usr/local/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.6m -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.6m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.6m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.6m -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.6m -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.6m -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.6m -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    g++ -pthread -shared -B /usr/local/compiler_compat -L/usr/local/lib -Wl,-rpath=/usr/local/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -L/usr/local/lib/python3.6/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'syncbn' extension\n",
            "    gcc -pthread -B /usr/local/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    g++ -pthread -shared -B /usr/local/compiler_compat -L/usr/local/lib -Wl,-rpath=/usr/local/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/lib/python3.6/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'fused_layer_norm_cuda' extension\n",
            "    gcc -pthread -B /usr/local/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm(at::Tensor, c10::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:129:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_affine(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:149:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:150:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:151:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘at::Tensor layer_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:193:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:194:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:195:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:196:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:218:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:219:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:220:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:221:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:222:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:223:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    g++ -pthread -shared -B /usr/local/compiler_compat -L/usr/local/lib -Wl,-rpath=/usr/local/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.6/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'mlp_cuda' extension\n",
            "    gcc -pthread -B /usr/local/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.6m -c csrc/mlp.cpp -o build/temp.linux-x86_64-3.6/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:56:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:64:77: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n",
            "                                                                                 ^\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
            "                                                                        ^\n",
            "    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:67:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:129:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                             \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:131:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                    \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:46:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:113:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:119:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < inputs.size(); i++) {\n",
            "                       ~~^~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:120:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:123:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:129:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                             \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:131:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                    \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:46:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/site-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/site-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.6m -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/autograd/profiler.h(100): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/autograd/profiler.h(115): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/autograd/profiler.h(100): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    /usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/autograd/profiler.h(115): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    g++ -pthread -shared -B /usr/local/compiler_compat -L/usr/local/lib -Wl,-rpath=/usr/local/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/mlp.o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -L/usr/local/lib/python3.6/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "    running install_lib\n",
            "    copying build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/site-packages\n",
            "    copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/site-packages\n",
            "    creating /usr/local/lib/python3.6/site-packages/apex\n",
            "    creating /usr/local/lib/python3.6/site-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> /usr/local/lib/python3.6/site-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> /usr/local/lib/python3.6/site-packages/apex/multi_tensor_apply\n",
            "    creating /usr/local/lib/python3.6/site-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> /usr/local/lib/python3.6/site-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> /usr/local/lib/python3.6/site-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> /usr/local/lib/python3.6/site-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/__init__.py -> /usr/local/lib/python3.6/site-packages/apex\n",
            "    creating /usr/local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> /usr/local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> /usr/local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> /usr/local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> /usr/local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> /usr/local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> /usr/local/lib/python3.6/site-packages/apex/amp\n",
            "    creating /usr/local/lib/python3.6/site-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> /usr/local/lib/python3.6/site-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> /usr/local/lib/python3.6/site-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> /usr/local/lib/python3.6/site-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> /usr/local/lib/python3.6/site-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> /usr/local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> /usr/local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> /usr/local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> /usr/local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> /usr/local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> /usr/local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> /usr/local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> /usr/local/lib/python3.6/site-packages/apex/amp\n",
            "    creating /usr/local/lib/python3.6/site-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/site-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> /usr/local/lib/python3.6/site-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_novograd.py -> /usr/local/lib/python3.6/site-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/site-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adagrad.py -> /usr/local/lib/python3.6/site-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> /usr/local/lib/python3.6/site-packages/apex/optimizers\n",
            "    creating /usr/local/lib/python3.6/site-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> /usr/local/lib/python3.6/site-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> /usr/local/lib/python3.6/site-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> /usr/local/lib/python3.6/site-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> /usr/local/lib/python3.6/site-packages/apex/RNN\n",
            "    creating /usr/local/lib/python3.6/site-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> /usr/local/lib/python3.6/site-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> /usr/local/lib/python3.6/site-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> /usr/local/lib/python3.6/site-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> /usr/local/lib/python3.6/site-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> /usr/local/lib/python3.6/site-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> /usr/local/lib/python3.6/site-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/site-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/site-packages/apex/parallel\n",
            "    creating /usr/local/lib/python3.6/site-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> /usr/local/lib/python3.6/site-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> /usr/local/lib/python3.6/site-packages/apex/normalization\n",
            "    creating /usr/local/lib/python3.6/site-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> /usr/local/lib/python3.6/site-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> /usr/local/lib/python3.6/site-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> /usr/local/lib/python3.6/site-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> /usr/local/lib/python3.6/site-packages/apex/fp16_utils\n",
            "    creating /usr/local/lib/python3.6/site-packages/apex/pyprof\n",
            "    creating /usr/local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__main__.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__init__.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/usage.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/optim.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/recurrentCell.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/misc.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/utility.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/reduction.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/linear.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/loss.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/embedding.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/prof.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/randomSample.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pointwise.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pooling.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/dropout.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/conv.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/convert.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/data.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/activation.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/softmax.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/blas.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/base.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/index_slice_join_mutate.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/output.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/normalization.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/__init__.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof\n",
            "    creating /usr/local/lib/python3.6/site-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/__init__.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/nvmarker.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/nvtx\n",
            "    creating /usr/local/lib/python3.6/site-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__main__.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__init__.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/db.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/parse.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/nvvp.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/kernel.py -> /usr/local/lib/python3.6/site-packages/apex/pyprof/parse\n",
            "    creating /usr/local/lib/python3.6/site-packages/apex/contrib\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/__init__.py -> /usr/local/lib/python3.6/site-packages/apex/contrib\n",
            "    creating /usr/local/lib/python3.6/site-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/__init__.py -> /usr/local/lib/python3.6/site-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/batch_norm.py -> /usr/local/lib/python3.6/site-packages/apex/contrib/groupbn\n",
            "    creating /usr/local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/__init__.py -> /usr/local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v3.py -> /usr/local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v2.py -> /usr/local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fp16_optimizer.py -> /usr/local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam.py -> /usr/local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_adam.py -> /usr/local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_lamb.py -> /usr/local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    creating /usr/local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/__init__.py -> /usr/local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn.py -> /usr/local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn_func.py -> /usr/local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn.py -> /usr/local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> /usr/local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> /usr/local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    creating /usr/local/lib/python3.6/site-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/__init__.py -> /usr/local/lib/python3.6/site-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/sparse_masklib.py -> /usr/local/lib/python3.6/site-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/asp.py -> /usr/local/lib/python3.6/site-packages/apex/contrib/sparsity\n",
            "    creating /usr/local/lib/python3.6/site-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/__init__.py -> /usr/local/lib/python3.6/site-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/softmax_xentropy.py -> /usr/local/lib/python3.6/site-packages/apex/contrib/xentropy\n",
            "    creating /usr/local/lib/python3.6/site-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/mlp/__init__.py -> /usr/local/lib/python3.6/site-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/mlp/mlp.py -> /usr/local/lib/python3.6/site-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/site-packages\n",
            "    copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/site-packages\n",
            "    copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/site-packages\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/amp/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/amp/wrap.py to wrap.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/amp/scaler.py to scaler.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/amp/__version__.py to __version__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/amp/utils.py to utils.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/amp/_initialize.py to _initialize.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/amp/handle.py to handle.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/amp/frontend.py to frontend.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/amp/amp.py to amp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/amp/opt.py to opt.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/amp/compat.py to compat.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/optimizers/fused_novograd.py to fused_novograd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/RNN/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/RNN/models.py to models.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/RNN/cells.py to cells.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/parallel/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/parallel/distributed.py to distributed.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/parallel/LARC.py to LARC.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/normalization/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/prof/__main__.py to __main__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/prof/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/prof/usage.py to usage.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/prof/optim.py to optim.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/prof/misc.py to misc.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/prof/utility.py to utility.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/prof/reduction.py to reduction.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/prof/linear.py to linear.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/prof/loss.py to loss.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/prof/embedding.py to embedding.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/prof/prof.py to prof.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/prof/randomSample.py to randomSample.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/prof/pointwise.py to pointwise.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/prof/pooling.py to pooling.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/prof/dropout.py to dropout.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/prof/conv.py to conv.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/prof/convert.py to convert.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/prof/data.py to data.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/prof/activation.py to activation.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/prof/softmax.py to softmax.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/prof/blas.py to blas.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/prof/base.py to base.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/prof/output.py to output.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/prof/normalization.py to normalization.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/nvtx/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/parse/__main__.py to __main__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/parse/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/parse/db.py to db.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/parse/parse.py to parse.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/parse/nvvp.py to nvvp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/pyprof/parse/kernel.py to kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/contrib/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/contrib/groupbn/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/contrib/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/contrib/optimizers/distributed_fused_adam_v3.py to distributed_fused_adam_v3.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/contrib/optimizers/distributed_fused_adam_v2.py to distributed_fused_adam_v2.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/contrib/multihead_attn/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/contrib/sparsity/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/contrib/sparsity/asp.py to asp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/contrib/xentropy/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/mlp/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/site-packages/apex/mlp/mlp.py to mlp.cpython-36.pyc\n",
            "    running install_egg_info\n",
            "    running egg_info\n",
            "    creating apex.egg-info\n",
            "    writing apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to apex.egg-info/top_level.txt\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    reading manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    Copying apex.egg-info to /usr/local/lib/python3.6/site-packages/apex-0.1-py3.6.egg-info\n",
            "    running install_scripts\n",
            "    writing list of installed files to '/tmp/pip-record-ff4fstv4/install-record.txt'\n",
            "    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n",
            "Successfully installed apex-0.1\n",
            "Removed build tracker: '/tmp/pip-req-tracker-x4gz8hvj'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkNescA_OLNG",
        "outputId": "254d08de-5318-4f39-b46a-087d912d5b0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "\n",
        "%cd \"/content/drive/My Drive/Colab Notebooks/unilm/layoutlm/apex\"\n",
        "!pip install ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/unilm/layoutlm/apex\n",
            "Processing /content/drive/My Drive/Colab Notebooks/unilm/layoutlm/apex\n",
            "Building wheels for collected packages: apex\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-py3-none-any.whl size=192846 sha256=43f2319237f8d7309e933d2132a8ba66c15898a4f30425abe5aa98518fc8f914\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8_0o41y0/wheels/5e/84/4d/ba6c36aff1f9b8f8ab3ce6be034248e01e816ec599711b7dfd\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "  Attempting uninstall: apex\n",
            "    Found existing installation: apex 0.1\n",
            "    Uninstalling apex-0.1:\n",
            "      Successfully uninstalled apex-0.1\n",
            "Successfully installed apex-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYkLQVKVrDqE",
        "outputId": "e736b518-e3f8-4281-843b-7467a51c3ccf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd \"/content/drive/My Drive/Colab Notebooks/unilm/layoutlm/examples/seq_labeling\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/unilm/layoutlm/examples/seq_labeling\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDsI4qxMsmdf",
        "outputId": "36c435a5-5d21-49b9-b737-6ee72baac85f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!sh preprocess.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-14 23:17:46--  https://guillaumejaume.github.io/FUNSD/dataset.zip\n",
            "Resolving guillaumejaume.github.io (guillaumejaume.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to guillaumejaume.github.io (guillaumejaume.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16838830 (16M) [application/zip]\n",
            "Saving to: ‘dataset.zip’\n",
            "\n",
            "dataset.zip         100%[===================>]  16.06M  50.3MB/s    in 0.3s    \n",
            "\n",
            "2020-10-14 23:17:46 (50.3 MB/s) - ‘dataset.zip’ saved [16838830/16838830]\n",
            "\n",
            "Archive:  dataset.zip\n",
            "   creating: dataset/\n",
            "   creating: dataset/training_data/\n",
            "  inflating: dataset/training_data/.DS_Store  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/dataset/\n",
            "   creating: __MACOSX/dataset/training_data/\n",
            "  inflating: __MACOSX/dataset/training_data/._.DS_Store  \n",
            "   creating: dataset/training_data/images/\n",
            "  inflating: dataset/training_data/images/92091873.png  \n",
            "   creating: __MACOSX/dataset/training_data/images/\n",
            "  inflating: __MACOSX/dataset/training_data/images/._92091873.png  \n",
            "  inflating: dataset/training_data/images/91939637.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._91939637.png  \n",
            "  inflating: dataset/training_data/images/87533049.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._87533049.png  \n",
            "  inflating: dataset/training_data/images/01073843.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._01073843.png  \n",
            "  inflating: dataset/training_data/images/92586242.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._92586242.png  \n",
            "  inflating: dataset/training_data/images/0012529284.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0012529284.png  \n",
            "  inflating: dataset/training_data/images/71341634.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._71341634.png  \n",
            "  inflating: dataset/training_data/images/0001477983.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0001477983.png  \n",
            "  inflating: dataset/training_data/images/91315069_91315070.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._91315069_91315070.png  \n",
            "  inflating: dataset/training_data/images/0060207528.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0060207528.png  \n",
            "  inflating: dataset/training_data/images/91161344_91161347.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._91161344_91161347.png  \n",
            "  inflating: dataset/training_data/images/71366499.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._71366499.png  \n",
            "  inflating: dataset/training_data/images/00922237.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._00922237.png  \n",
            "  inflating: dataset/training_data/images/91355841.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._91355841.png  \n",
            "  inflating: dataset/training_data/images/93380187.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._93380187.png  \n",
            "  inflating: dataset/training_data/images/01150773_01150774.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._01150773_01150774.png  \n",
            "  inflating: dataset/training_data/images/0060165115.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0060165115.png  \n",
            "  inflating: dataset/training_data/images/91914407.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._91914407.png  \n",
            "  inflating: dataset/training_data/images/92081358_1359.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._92081358_1359.png  \n",
            "  inflating: dataset/training_data/images/660978.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._660978.png  \n",
            "  inflating: dataset/training_data/images/0011974919.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0011974919.png  \n",
            "  inflating: dataset/training_data/images/81749056_9057.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._81749056_9057.png  \n",
            "  inflating: dataset/training_data/images/00093726.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._00093726.png  \n",
            "  inflating: dataset/training_data/images/0000999294.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0000999294.png  \n",
            "  inflating: dataset/training_data/images/81186212.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._81186212.png  \n",
            "  inflating: dataset/training_data/images/0060077689.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0060077689.png  \n",
            "  inflating: dataset/training_data/images/87672097.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._87672097.png  \n",
            "  inflating: dataset/training_data/images/11875011.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._11875011.png  \n",
            "  inflating: dataset/training_data/images/00851772_1780.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._00851772_1780.png  \n",
            "  inflating: dataset/training_data/images/0011859695.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0011859695.png  \n",
            "  inflating: dataset/training_data/images/81619511_9513.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._81619511_9513.png  \n",
            "  inflating: dataset/training_data/images/0001456787.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0001456787.png  \n",
            "  inflating: dataset/training_data/images/81310636.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._81310636.png  \n",
            "  inflating: dataset/training_data/images/0060080406.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0060080406.png  \n",
            "  inflating: dataset/training_data/images/0011973451.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0011973451.png  \n",
            "  inflating: dataset/training_data/images/91104867.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._91104867.png  \n",
            "  inflating: dataset/training_data/images/00836244.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._00836244.png  \n",
            "  inflating: dataset/training_data/images/0001476912.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0001476912.png  \n",
            "  inflating: dataset/training_data/images/12052385.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._12052385.png  \n",
            "  inflating: dataset/training_data/images/0060255888.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0060255888.png  \n",
            "  inflating: dataset/training_data/images/71206427.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._71206427.png  \n",
            "  inflating: dataset/training_data/images/0012199830.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0012199830.png  \n",
            "  inflating: dataset/training_data/images/0001463448.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0001463448.png  \n",
            "  inflating: dataset/training_data/images/0000990274.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0000990274.png  \n",
            "  inflating: dataset/training_data/images/716552.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._716552.png  \n",
            "  inflating: dataset/training_data/images/00920222.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._00920222.png  \n",
            "  inflating: dataset/training_data/images/0012529295.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0012529295.png  \n",
            "  inflating: dataset/training_data/images/80728670.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._80728670.png  \n",
            "  inflating: dataset/training_data/images/93213298.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._93213298.png  \n",
            "  inflating: dataset/training_data/images/71108371.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._71108371.png  \n",
            "  inflating: dataset/training_data/images/0001209043.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0001209043.png  \n",
            "  inflating: dataset/training_data/images/88057519.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._88057519.png  \n",
            "  inflating: dataset/training_data/images/01191071_1072.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._01191071_1072.png  \n",
            "  inflating: dataset/training_data/images/00836816.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._00836816.png  \n",
            "  inflating: dataset/training_data/images/91361993.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._91361993.png  \n",
            "  inflating: dataset/training_data/images/92298125.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._92298125.png  \n",
            "  inflating: dataset/training_data/images/00040534.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._00040534.png  \n",
            "  inflating: dataset/training_data/images/0011838621.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0011838621.png  \n",
            "  inflating: dataset/training_data/images/0060029036.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0060029036.png  \n",
            "  inflating: dataset/training_data/images/0000989556.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0000989556.png  \n",
            "  inflating: dataset/training_data/images/0060024314.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0060024314.png  \n",
            "  inflating: dataset/training_data/images/0001129658.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0001129658.png  \n",
            "  inflating: dataset/training_data/images/0071032790.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0071032790.png  \n",
            "  inflating: dataset/training_data/images/00838511_00838525.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._00838511_00838525.png  \n",
            "  inflating: dataset/training_data/images/87682908.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._87682908.png  \n",
            "  inflating: dataset/training_data/images/0060214859.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0060214859.png  \n",
            "  inflating: dataset/training_data/images/00070353.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._00070353.png  \n",
            "  inflating: dataset/training_data/images/93351929_93351931.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._93351929_93351931.png  \n",
            "  inflating: dataset/training_data/images/0001438955.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0001438955.png  \n",
            "  inflating: dataset/training_data/images/81619486_9488.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._81619486_9488.png  \n",
            "  inflating: dataset/training_data/images/00920294.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._00920294.png  \n",
            "  inflating: dataset/training_data/images/0000971160.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0000971160.png  \n",
            "  inflating: dataset/training_data/images/0012947358.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0012947358.png  \n",
            "  inflating: dataset/training_data/images/00865872.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._00865872.png  \n",
            "  inflating: dataset/training_data/images/91391286.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._91391286.png  \n",
            "  inflating: dataset/training_data/images/01122115.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._01122115.png  \n",
            "  inflating: dataset/training_data/images/01408099_01408101.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._01408099_01408101.png  \n",
            "  inflating: dataset/training_data/images/80310840a.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._80310840a.png  \n",
            "  inflating: dataset/training_data/images/0012602424.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0012602424.png  \n",
            "  inflating: dataset/training_data/images/0071032807.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0071032807.png  \n",
            "  inflating: dataset/training_data/images/0060308251.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0060308251.png  \n",
            "  inflating: dataset/training_data/images/92094746.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._92094746.png  \n",
            "  inflating: dataset/training_data/images/0060302201.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0060302201.png  \n",
            "  inflating: dataset/training_data/images/0060094595.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0060094595.png  \n",
            "  inflating: dataset/training_data/images/71190280.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._71190280.png  \n",
            "  inflating: dataset/training_data/images/92433599_92433601.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._92433599_92433601.png  \n",
            "  inflating: dataset/training_data/images/0060091229.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0060091229.png  \n",
            "  inflating: dataset/training_data/images/89386032.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._89386032.png  \n",
            "  inflating: dataset/training_data/images/91974562.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._91974562.png  \n",
            "  inflating: dataset/training_data/images/92094751.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._92094751.png  \n",
            "  inflating: dataset/training_data/images/89817999_8002.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._89817999_8002.png  \n",
            "  inflating: dataset/training_data/images/71601299.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._71601299.png  \n",
            "  inflating: dataset/training_data/images/00851879.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._00851879.png  \n",
            "  inflating: dataset/training_data/images/92039708_9710.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._92039708_9710.png  \n",
            "  inflating: dataset/training_data/images/92657391.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._92657391.png  \n",
            "  inflating: dataset/training_data/images/80718412_8413.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._80718412_8413.png  \n",
            "  inflating: dataset/training_data/images/00860012_00860014.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._00860012_00860014.png  \n",
            "  inflating: dataset/training_data/images/93329540.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._93329540.png  \n",
            "  inflating: dataset/training_data/images/0060068489.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0060068489.png  \n",
            "  inflating: dataset/training_data/images/12603270.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._12603270.png  \n",
            "  inflating: dataset/training_data/images/0060308461.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0060308461.png  \n",
            "  inflating: dataset/training_data/images/0060270727.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0060270727.png  \n",
            "  inflating: dataset/training_data/images/00837285.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._00837285.png  \n",
            "  inflating: dataset/training_data/images/91356315.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._91356315.png  \n",
            "  inflating: dataset/training_data/images/0011899960.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0011899960.png  \n",
            "  inflating: dataset/training_data/images/82254638.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._82254638.png  \n",
            "  inflating: dataset/training_data/images/0001118259.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0001118259.png  \n",
            "  inflating: dataset/training_data/images/0011906503.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0011906503.png  \n",
            "  inflating: dataset/training_data/images/91391310.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._91391310.png  \n",
            "  inflating: dataset/training_data/images/01197604.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._01197604.png  \n",
            "  inflating: dataset/training_data/images/80707440_7443.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._80707440_7443.png  \n",
            "  inflating: dataset/training_data/images/11508234.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._11508234.png  \n",
            "  inflating: dataset/training_data/images/0011845203.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0011845203.png  \n",
            "  inflating: dataset/training_data/images/0060262650.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0060262650.png  \n",
            "  inflating: dataset/training_data/images/0060173256.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0060173256.png  \n",
            "  inflating: dataset/training_data/images/88547278_88547279.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._88547278_88547279.png  \n",
            "  inflating: dataset/training_data/images/0001123541.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0001123541.png  \n",
            "  inflating: dataset/training_data/images/81574683.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._81574683.png  \n",
            "  inflating: dataset/training_data/images/0001239897.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0001239897.png  \n",
            "  inflating: dataset/training_data/images/0011856542.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0011856542.png  \n",
            "  inflating: dataset/training_data/images/0060025670.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0060025670.png  \n",
            "  inflating: dataset/training_data/images/89867723.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._89867723.png  \n",
            "  inflating: dataset/training_data/images/0012178355.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0012178355.png  \n",
            "  inflating: dataset/training_data/images/91856041_6049.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._91856041_6049.png  \n",
            "  inflating: dataset/training_data/images/93455715.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._93455715.png  \n",
            "  inflating: dataset/training_data/images/0011976929.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0011976929.png  \n",
            "  inflating: dataset/training_data/images/0060007216.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0060007216.png  \n",
            "  inflating: dataset/training_data/images/0001485288.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0001485288.png  \n",
            "  inflating: dataset/training_data/images/0001463282.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0001463282.png  \n",
            "  inflating: dataset/training_data/images/0030041455.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0030041455.png  \n",
            "  inflating: dataset/training_data/images/71202511.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._71202511.png  \n",
            "  inflating: dataset/training_data/images/92327794.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._92327794.png  \n",
            "  inflating: dataset/training_data/images/0060136394.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0060136394.png  \n",
            "  inflating: dataset/training_data/images/89368010.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._89368010.png  \n",
            "  inflating: dataset/training_data/images/92314414.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._92314414.png  \n",
            "  inflating: dataset/training_data/images/92657311_7313.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._92657311_7313.png  \n",
            "  inflating: dataset/training_data/images/13149651.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._13149651.png  \n",
            "  inflating: dataset/training_data/images/91903177.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._91903177.png  \n",
            "  inflating: dataset/training_data/images/71563825.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._71563825.png  \n",
            "  inflating: dataset/training_data/images/00283813.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._00283813.png  \n",
            "  inflating: dataset/training_data/images/00866042.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._00866042.png  \n",
            "  inflating: dataset/training_data/images/0060036622.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0060036622.png  \n",
            "  inflating: dataset/training_data/images/0011505151.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0011505151.png  \n",
            "  inflating: dataset/training_data/images/0013255595.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0013255595.png  \n",
            "  inflating: dataset/training_data/images/0030031163.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0030031163.png  \n",
            "  inflating: dataset/training_data/images/91581919.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._91581919.png  \n",
            "  inflating: dataset/training_data/images/91372360.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._91372360.png  \n",
            "  inflating: dataset/training_data/images/12825369.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._12825369.png  \n",
            "  inflating: dataset/training_data/images/0060000813.png  \n",
            "  inflating: __MACOSX/dataset/training_data/images/._0060000813.png  \n",
            "  inflating: __MACOSX/dataset/training_data/._images  \n",
            "   creating: dataset/training_data/annotations/\n",
            "  inflating: dataset/training_data/annotations/71206427.json  \n",
            "   creating: __MACOSX/dataset/training_data/annotations/\n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._71206427.json  \n",
            "  inflating: dataset/training_data/annotations/0001123541.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0001123541.json  \n",
            "  inflating: dataset/training_data/annotations/92039708_9710.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._92039708_9710.json  \n",
            "  inflating: dataset/training_data/annotations/0060029036.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0060029036.json  \n",
            "  inflating: dataset/training_data/annotations/81619511_9513.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._81619511_9513.json  \n",
            "  inflating: dataset/training_data/annotations/89386032.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._89386032.json  \n",
            "  inflating: dataset/training_data/annotations/0001209043.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0001209043.json  \n",
            "  inflating: dataset/training_data/annotations/0060262650.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0060262650.json  \n",
            "  inflating: dataset/training_data/annotations/0001129658.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0001129658.json  \n",
            "  inflating: dataset/training_data/annotations/0060165115.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0060165115.json  \n",
            "  inflating: dataset/training_data/annotations/0060007216.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0060007216.json  \n",
            "  inflating: dataset/training_data/annotations/00093726.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._00093726.json  \n",
            "  inflating: dataset/training_data/annotations/00836244.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._00836244.json  \n",
            "  inflating: dataset/training_data/annotations/0060094595.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0060094595.json  \n",
            "  inflating: dataset/training_data/annotations/80728670.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._80728670.json  \n",
            "  inflating: dataset/training_data/annotations/92327794.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._92327794.json  \n",
            "  inflating: dataset/training_data/annotations/00922237.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._00922237.json  \n",
            "  inflating: dataset/training_data/annotations/92298125.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._92298125.json  \n",
            "  inflating: dataset/training_data/annotations/12825369.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._12825369.json  \n",
            "  inflating: dataset/training_data/annotations/92586242.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._92586242.json  \n",
            "  inflating: dataset/training_data/annotations/0001463448.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0001463448.json  \n",
            "  inflating: dataset/training_data/annotations/0060302201.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0060302201.json  \n",
            "  inflating: dataset/training_data/annotations/91391286.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._91391286.json  \n",
            "  inflating: dataset/training_data/annotations/0060308461.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0060308461.json  \n",
            "  inflating: dataset/training_data/annotations/81619486_9488.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._81619486_9488.json  \n",
            "  inflating: dataset/training_data/annotations/0071032790.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0071032790.json  \n",
            "  inflating: dataset/training_data/annotations/0060080406.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0060080406.json  \n",
            "  inflating: dataset/training_data/annotations/0012529284.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0012529284.json  \n",
            "  inflating: dataset/training_data/annotations/0060036622.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0060036622.json  \n",
            "  inflating: dataset/training_data/annotations/71190280.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._71190280.json  \n",
            "  inflating: dataset/training_data/annotations/93455715.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._93455715.json  \n",
            "  inflating: dataset/training_data/annotations/87682908.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._87682908.json  \n",
            "  inflating: dataset/training_data/annotations/91356315.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._91356315.json  \n",
            "  inflating: dataset/training_data/annotations/87533049.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._87533049.json  \n",
            "  inflating: dataset/training_data/annotations/0060308251.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0060308251.json  \n",
            "  inflating: dataset/training_data/annotations/01122115.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._01122115.json  \n",
            "  inflating: dataset/training_data/annotations/71601299.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._71601299.json  \n",
            "  inflating: dataset/training_data/annotations/93329540.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._93329540.json  \n",
            "  inflating: dataset/training_data/annotations/0000990274.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0000990274.json  \n",
            "  inflating: dataset/training_data/annotations/71563825.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._71563825.json  \n",
            "  inflating: dataset/training_data/annotations/0060255888.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0060255888.json  \n",
            "  inflating: dataset/training_data/annotations/0000989556.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0000989556.json  \n",
            "  inflating: dataset/training_data/annotations/0011505151.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0011505151.json  \n",
            "  inflating: dataset/training_data/annotations/00070353.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._00070353.json  \n",
            "  inflating: dataset/training_data/annotations/0012529295.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0012529295.json  \n",
            "  inflating: dataset/training_data/annotations/81574683.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._81574683.json  \n",
            "  inflating: dataset/training_data/annotations/0060136394.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0060136394.json  \n",
            "  inflating: dataset/training_data/annotations/0030031163.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0030031163.json  \n",
            "  inflating: dataset/training_data/annotations/81749056_9057.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._81749056_9057.json  \n",
            "  inflating: dataset/training_data/annotations/660978.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._660978.json  \n",
            "  inflating: dataset/training_data/annotations/0060214859.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0060214859.json  \n",
            "  inflating: dataset/training_data/annotations/92091873.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._92091873.json  \n",
            "  inflating: dataset/training_data/annotations/71202511.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._71202511.json  \n",
            "  inflating: dataset/training_data/annotations/92657391.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._92657391.json  \n",
            "  inflating: dataset/training_data/annotations/11508234.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._11508234.json  \n",
            "  inflating: dataset/training_data/annotations/0011973451.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0011973451.json  \n",
            "  inflating: dataset/training_data/annotations/91355841.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._91355841.json  \n",
            "  inflating: dataset/training_data/annotations/0001476912.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0001476912.json  \n",
            "  inflating: dataset/training_data/annotations/00920294.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._00920294.json  \n",
            "  inflating: dataset/training_data/annotations/0001438955.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0001438955.json  \n",
            "  inflating: dataset/training_data/annotations/01073843.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._01073843.json  \n",
            "  inflating: dataset/training_data/annotations/0011856542.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0011856542.json  \n",
            "  inflating: dataset/training_data/annotations/93213298.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._93213298.json  \n",
            "  inflating: dataset/training_data/annotations/0012178355.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0012178355.json  \n",
            "  inflating: dataset/training_data/annotations/89867723.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._89867723.json  \n",
            "  inflating: dataset/training_data/annotations/92433599_92433601.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._92433599_92433601.json  \n",
            "  inflating: dataset/training_data/annotations/87672097.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._87672097.json  \n",
            "  inflating: dataset/training_data/annotations/91315069_91315070.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._91315069_91315070.json  \n",
            "  inflating: dataset/training_data/annotations/0060077689.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0060077689.json  \n",
            "  inflating: dataset/training_data/annotations/0060000813.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0060000813.json  \n",
            "  inflating: dataset/training_data/annotations/92081358_1359.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._92081358_1359.json  \n",
            "  inflating: dataset/training_data/annotations/0071032807.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0071032807.json  \n",
            "  inflating: dataset/training_data/annotations/01197604.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._01197604.json  \n",
            "  inflating: dataset/training_data/annotations/71366499.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._71366499.json  \n",
            "  inflating: dataset/training_data/annotations/11875011.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._11875011.json  \n",
            "  inflating: dataset/training_data/annotations/91104867.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._91104867.json  \n",
            "  inflating: dataset/training_data/annotations/91391310.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._91391310.json  \n",
            "  inflating: dataset/training_data/annotations/0011899960.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0011899960.json  \n",
            "  inflating: dataset/training_data/annotations/81186212.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._81186212.json  \n",
            "  inflating: dataset/training_data/annotations/82254638.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._82254638.json  \n",
            "  inflating: dataset/training_data/annotations/01150773_01150774.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._01150773_01150774.json  \n",
            "  inflating: dataset/training_data/annotations/00920222.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._00920222.json  \n",
            "  inflating: dataset/training_data/annotations/92314414.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._92314414.json  \n",
            "  inflating: dataset/training_data/annotations/00838511_00838525.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._00838511_00838525.json  \n",
            "  inflating: dataset/training_data/annotations/716552.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._716552.json  \n",
            "  inflating: dataset/training_data/annotations/91372360.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._91372360.json  \n",
            "  inflating: dataset/training_data/annotations/71341634.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._71341634.json  \n",
            "  inflating: dataset/training_data/annotations/91856041_6049.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._91856041_6049.json  \n",
            "  inflating: dataset/training_data/annotations/00040534.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._00040534.json  \n",
            "  inflating: dataset/training_data/annotations/89368010.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._89368010.json  \n",
            "  inflating: dataset/training_data/annotations/91914407.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._91914407.json  \n",
            "  inflating: dataset/training_data/annotations/0060024314.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0060024314.json  \n",
            "  inflating: dataset/training_data/annotations/00866042.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._00866042.json  \n",
            "  inflating: dataset/training_data/annotations/12603270.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._12603270.json  \n",
            "  inflating: dataset/training_data/annotations/0012602424.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0012602424.json  \n",
            "  inflating: dataset/training_data/annotations/80718412_8413.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._80718412_8413.json  \n",
            "  inflating: dataset/training_data/annotations/88547278_88547279.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._88547278_88547279.json  \n",
            "  inflating: dataset/training_data/annotations/0000971160.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0000971160.json  \n",
            "  inflating: dataset/training_data/annotations/13149651.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._13149651.json  \n",
            "  inflating: dataset/training_data/annotations/0060091229.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0060091229.json  \n",
            "  inflating: dataset/training_data/annotations/91939637.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._91939637.json  \n",
            "  inflating: dataset/training_data/annotations/0011859695.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0011859695.json  \n",
            "  inflating: dataset/training_data/annotations/00851772_1780.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._00851772_1780.json  \n",
            "  inflating: dataset/training_data/annotations/80707440_7443.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._80707440_7443.json  \n",
            "  inflating: dataset/training_data/annotations/0012947358.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0012947358.json  \n",
            "  inflating: dataset/training_data/annotations/88057519.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._88057519.json  \n",
            "  inflating: dataset/training_data/annotations/71108371.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._71108371.json  \n",
            "  inflating: dataset/training_data/annotations/91903177.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._91903177.json  \n",
            "  inflating: dataset/training_data/annotations/91581919.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._91581919.json  \n",
            "  inflating: dataset/training_data/annotations/0013255595.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0013255595.json  \n",
            "  inflating: dataset/training_data/annotations/92657311_7313.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._92657311_7313.json  \n",
            "  inflating: dataset/training_data/annotations/0060173256.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0060173256.json  \n",
            "  inflating: dataset/training_data/annotations/0001477983.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0001477983.json  \n",
            "  inflating: dataset/training_data/annotations/92094751.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._92094751.json  \n",
            "  inflating: dataset/training_data/annotations/0030041455.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0030041455.json  \n",
            "  inflating: dataset/training_data/annotations/93380187.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._93380187.json  \n",
            "  inflating: dataset/training_data/annotations/93351929_93351931.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._93351929_93351931.json  \n",
            "  inflating: dataset/training_data/annotations/01191071_1072.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._01191071_1072.json  \n",
            "  inflating: dataset/training_data/annotations/0011976929.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0011976929.json  \n",
            "  inflating: dataset/training_data/annotations/0060207528.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0060207528.json  \n",
            "  inflating: dataset/training_data/annotations/0000999294.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0000999294.json  \n",
            "  inflating: dataset/training_data/annotations/00851879.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._00851879.json  \n",
            "  inflating: dataset/training_data/annotations/0001485288.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0001485288.json  \n",
            "  inflating: dataset/training_data/annotations/0012199830.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0012199830.json  \n",
            "  inflating: dataset/training_data/annotations/00865872.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._00865872.json  \n",
            "  inflating: dataset/training_data/annotations/91361993.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._91361993.json  \n",
            "  inflating: dataset/training_data/annotations/00283813.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._00283813.json  \n",
            "  inflating: dataset/training_data/annotations/0001118259.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0001118259.json  \n",
            "  inflating: dataset/training_data/annotations/91974562.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._91974562.json  \n",
            "  inflating: dataset/training_data/annotations/0060025670.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0060025670.json  \n",
            "  inflating: dataset/training_data/annotations/12052385.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._12052385.json  \n",
            "  inflating: dataset/training_data/annotations/0060068489.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0060068489.json  \n",
            "  inflating: dataset/training_data/annotations/0001456787.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0001456787.json  \n",
            "  inflating: dataset/training_data/annotations/91161344_91161347.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._91161344_91161347.json  \n",
            "  inflating: dataset/training_data/annotations/0060270727.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0060270727.json  \n",
            "  inflating: dataset/training_data/annotations/01408099_01408101.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._01408099_01408101.json  \n",
            "  inflating: dataset/training_data/annotations/0011974919.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0011974919.json  \n",
            "  inflating: dataset/training_data/annotations/00837285.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._00837285.json  \n",
            "  inflating: dataset/training_data/annotations/89817999_8002.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._89817999_8002.json  \n",
            "  inflating: dataset/training_data/annotations/00836816.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._00836816.json  \n",
            "  inflating: dataset/training_data/annotations/0001239897.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0001239897.json  \n",
            "  inflating: dataset/training_data/annotations/81310636.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._81310636.json  \n",
            "  inflating: dataset/training_data/annotations/92094746.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._92094746.json  \n",
            "  inflating: dataset/training_data/annotations/0011838621.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0011838621.json  \n",
            "  inflating: dataset/training_data/annotations/00860012_00860014.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._00860012_00860014.json  \n",
            "  inflating: dataset/training_data/annotations/0011845203.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0011845203.json  \n",
            "  inflating: dataset/training_data/annotations/0011906503.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0011906503.json  \n",
            "  inflating: dataset/training_data/annotations/0001463282.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._0001463282.json  \n",
            "  inflating: dataset/training_data/annotations/80310840a.json  \n",
            "  inflating: __MACOSX/dataset/training_data/annotations/._80310840a.json  \n",
            "  inflating: __MACOSX/dataset/training_data/._annotations  \n",
            "  inflating: __MACOSX/dataset/._training_data  \n",
            "   creating: dataset/testing_data/\n",
            "  inflating: dataset/testing_data/.DS_Store  \n",
            "   creating: __MACOSX/dataset/testing_data/\n",
            "  inflating: __MACOSX/dataset/testing_data/._.DS_Store  \n",
            "   creating: dataset/testing_data/images/\n",
            "  inflating: dataset/testing_data/images/82837252.png  \n",
            "   creating: __MACOSX/dataset/testing_data/images/\n",
            "  inflating: __MACOSX/dataset/testing_data/images/._82837252.png  \n",
            "  inflating: dataset/testing_data/images/85201976.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._85201976.png  \n",
            "  inflating: dataset/testing_data/images/86263525.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._86263525.png  \n",
            "  inflating: dataset/testing_data/images/82251504.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._82251504.png  \n",
            "  inflating: dataset/testing_data/images/93106788.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._93106788.png  \n",
            "  inflating: dataset/testing_data/images/82573104.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._82573104.png  \n",
            "  inflating: dataset/testing_data/images/87528321.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._87528321.png  \n",
            "  inflating: dataset/testing_data/images/85240939.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._85240939.png  \n",
            "  inflating: dataset/testing_data/images/82562350.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._82562350.png  \n",
            "  inflating: dataset/testing_data/images/82253245_3247.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._82253245_3247.png  \n",
            "  inflating: dataset/testing_data/images/87093315_87093318.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._87093315_87093318.png  \n",
            "  inflating: dataset/testing_data/images/83443897.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._83443897.png  \n",
            "  inflating: dataset/testing_data/images/87332450.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._87332450.png  \n",
            "  inflating: dataset/testing_data/images/89856243.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._89856243.png  \n",
            "  inflating: dataset/testing_data/images/83635935.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._83635935.png  \n",
            "  inflating: dataset/testing_data/images/82250337_0338.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._82250337_0338.png  \n",
            "  inflating: dataset/testing_data/images/82200067_0069.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._82200067_0069.png  \n",
            "  inflating: dataset/testing_data/images/92380595.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._92380595.png  \n",
            "  inflating: dataset/testing_data/images/86236474_6476.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._86236474_6476.png  \n",
            "  inflating: dataset/testing_data/images/82491256.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._82491256.png  \n",
            "  inflating: dataset/testing_data/images/85540866.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._85540866.png  \n",
            "  inflating: dataset/testing_data/images/86244113.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._86244113.png  \n",
            "  inflating: dataset/testing_data/images/83823750.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._83823750.png  \n",
            "  inflating: dataset/testing_data/images/83594639.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._83594639.png  \n",
            "  inflating: dataset/testing_data/images/87528380.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._87528380.png  \n",
            "  inflating: dataset/testing_data/images/86220490.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._86220490.png  \n",
            "  inflating: dataset/testing_data/images/87086073.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._87086073.png  \n",
            "  inflating: dataset/testing_data/images/87147607.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._87147607.png  \n",
            "  inflating: dataset/testing_data/images/82252956_2958.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._82252956_2958.png  \n",
            "  inflating: dataset/testing_data/images/83641919_1921.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._83641919_1921.png  \n",
            "  inflating: dataset/testing_data/images/82253058_3059.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._82253058_3059.png  \n",
            "  inflating: dataset/testing_data/images/87125460.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._87125460.png  \n",
            "  inflating: dataset/testing_data/images/83624198.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._83624198.png  \n",
            "  inflating: dataset/testing_data/images/82504862.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._82504862.png  \n",
            "  inflating: dataset/testing_data/images/86328049_8050.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._86328049_8050.png  \n",
            "  inflating: dataset/testing_data/images/82254765.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._82254765.png  \n",
            "  inflating: dataset/testing_data/images/86079776_9777.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._86079776_9777.png  \n",
            "  inflating: dataset/testing_data/images/83553333_3334.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._83553333_3334.png  \n",
            "  inflating: dataset/testing_data/images/83573282.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._83573282.png  \n",
            "  inflating: dataset/testing_data/images/87137840.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._87137840.png  \n",
            "  inflating: dataset/testing_data/images/87428306.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._87428306.png  \n",
            "  inflating: dataset/testing_data/images/86230203_0206.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._86230203_0206.png  \n",
            "  inflating: dataset/testing_data/images/87594142_87594144.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._87594142_87594144.png  \n",
            "  inflating: dataset/testing_data/images/91814768_91814769.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._91814768_91814769.png  \n",
            "  inflating: dataset/testing_data/images/86075409_5410.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._86075409_5410.png  \n",
            "  inflating: dataset/testing_data/images/83772145.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._83772145.png  \n",
            "  inflating: dataset/testing_data/images/82253362_3364.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._82253362_3364.png  \n",
            "  inflating: dataset/testing_data/images/85629964.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._85629964.png  \n",
            "  inflating: dataset/testing_data/images/82092117.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._82092117.png  \n",
            "  inflating: dataset/testing_data/images/83996357.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/images/._83996357.png  \n",
            "  inflating: __MACOSX/dataset/testing_data/._images  \n",
            "   creating: dataset/testing_data/annotations/\n",
            "  inflating: dataset/testing_data/annotations/83594639.json  \n",
            "   creating: __MACOSX/dataset/testing_data/annotations/\n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._83594639.json  \n",
            "  inflating: dataset/testing_data/annotations/86236474_6476.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._86236474_6476.json  \n",
            "  inflating: dataset/testing_data/annotations/83553333_3334.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._83553333_3334.json  \n",
            "  inflating: dataset/testing_data/annotations/82200067_0069.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._82200067_0069.json  \n",
            "  inflating: dataset/testing_data/annotations/82504862.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._82504862.json  \n",
            "  inflating: dataset/testing_data/annotations/87086073.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._87086073.json  \n",
            "  inflating: dataset/testing_data/annotations/89856243.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._89856243.json  \n",
            "  inflating: dataset/testing_data/annotations/85629964.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._85629964.json  \n",
            "  inflating: dataset/testing_data/annotations/83996357.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._83996357.json  \n",
            "  inflating: dataset/testing_data/annotations/86079776_9777.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._86079776_9777.json  \n",
            "  inflating: dataset/testing_data/annotations/83772145.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._83772145.json  \n",
            "  inflating: dataset/testing_data/annotations/82252956_2958.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._82252956_2958.json  \n",
            "  inflating: dataset/testing_data/annotations/87528321.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._87528321.json  \n",
            "  inflating: dataset/testing_data/annotations/82250337_0338.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._82250337_0338.json  \n",
            "  inflating: dataset/testing_data/annotations/83635935.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._83635935.json  \n",
            "  inflating: dataset/testing_data/annotations/82254765.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._82254765.json  \n",
            "  inflating: dataset/testing_data/annotations/82573104.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._82573104.json  \n",
            "  inflating: dataset/testing_data/annotations/82253058_3059.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._82253058_3059.json  \n",
            "  inflating: dataset/testing_data/annotations/87428306.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._87428306.json  \n",
            "  inflating: dataset/testing_data/annotations/82253362_3364.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._82253362_3364.json  \n",
            "  inflating: dataset/testing_data/annotations/87137840.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._87137840.json  \n",
            "  inflating: dataset/testing_data/annotations/87147607.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._87147607.json  \n",
            "  inflating: dataset/testing_data/annotations/82251504.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._82251504.json  \n",
            "  inflating: dataset/testing_data/annotations/83823750.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._83823750.json  \n",
            "  inflating: dataset/testing_data/annotations/82253245_3247.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._82253245_3247.json  \n",
            "  inflating: dataset/testing_data/annotations/92380595.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._92380595.json  \n",
            "  inflating: dataset/testing_data/annotations/85201976.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._85201976.json  \n",
            "  inflating: dataset/testing_data/annotations/83443897.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._83443897.json  \n",
            "  inflating: dataset/testing_data/annotations/85240939.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._85240939.json  \n",
            "  inflating: dataset/testing_data/annotations/87332450.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._87332450.json  \n",
            "  inflating: dataset/testing_data/annotations/87528380.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._87528380.json  \n",
            "  inflating: dataset/testing_data/annotations/91814768_91814769.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._91814768_91814769.json  \n",
            "  inflating: dataset/testing_data/annotations/82562350.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._82562350.json  \n",
            "  inflating: dataset/testing_data/annotations/86244113.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._86244113.json  \n",
            "  inflating: dataset/testing_data/annotations/82491256.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._82491256.json  \n",
            "  inflating: dataset/testing_data/annotations/86328049_8050.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._86328049_8050.json  \n",
            "  inflating: dataset/testing_data/annotations/82837252.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._82837252.json  \n",
            "  inflating: dataset/testing_data/annotations/83641919_1921.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._83641919_1921.json  \n",
            "  inflating: dataset/testing_data/annotations/83573282.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._83573282.json  \n",
            "  inflating: dataset/testing_data/annotations/86220490.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._86220490.json  \n",
            "  inflating: dataset/testing_data/annotations/85540866.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._85540866.json  \n",
            "  inflating: dataset/testing_data/annotations/86263525.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._86263525.json  \n",
            "  inflating: dataset/testing_data/annotations/82092117.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._82092117.json  \n",
            "  inflating: dataset/testing_data/annotations/93106788.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._93106788.json  \n",
            "  inflating: dataset/testing_data/annotations/86230203_0206.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._86230203_0206.json  \n",
            "  inflating: dataset/testing_data/annotations/87594142_87594144.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._87594142_87594144.json  \n",
            "  inflating: dataset/testing_data/annotations/87093315_87093318.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._87093315_87093318.json  \n",
            "  inflating: dataset/testing_data/annotations/83624198.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._83624198.json  \n",
            "  inflating: dataset/testing_data/annotations/86075409_5410.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._86075409_5410.json  \n",
            "  inflating: dataset/testing_data/annotations/87125460.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/annotations/._87125460.json  \n",
            "  inflating: __MACOSX/dataset/testing_data/._annotations  \n",
            "  inflating: __MACOSX/dataset/._testing_data  \n",
            "  inflating: dataset/.DS_Store       \n",
            "  inflating: __MACOSX/dataset/._.DS_Store  \n",
            "  inflating: __MACOSX/._dataset      \n",
            "cut: the delimiter must be a single character\n",
            "Try 'cut --help' for more information.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHcNbABh4z8B",
        "outputId": "e3bde5c9-5ac5-4958-cddb-41ed6195e918",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        }
      },
      "source": [
        "%cd \"/content/drive/My Drive/Colab Notebooks/unilm/layoutlm/\"\n",
        "!pip install ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/unilm/layoutlm\n",
            "Processing /content/drive/My Drive/Colab Notebooks/unilm/layoutlm\n",
            "Requirement already satisfied: transformers==2.9.0 in /usr/local/lib/python3.6/site-packages (from layoutlm==0.0) (2.9.0)\n",
            "Requirement already satisfied: tensorboardX==2.0 in /usr/local/lib/python3.6/site-packages (from layoutlm==0.0) (2.0)\n",
            "Requirement already satisfied: lxml==4.5.1 in /usr/local/lib/python3.6/site-packages (from layoutlm==0.0) (4.5.1)\n",
            "Requirement already satisfied: seqeval==0.0.12 in /usr/local/lib/python3.6/site-packages (from layoutlm==0.0) (0.0.12)\n",
            "Requirement already satisfied: Pillow==7.1.2 in /usr/local/lib/python3.6/site-packages (from layoutlm==0.0) (7.1.2)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/site-packages (from transformers==2.9.0->layoutlm==0.0) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/site-packages (from transformers==2.9.0->layoutlm==0.0) (2.24.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/site-packages (from transformers==2.9.0->layoutlm==0.0) (0.1.91)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (from transformers==2.9.0->layoutlm==0.0) (1.19.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/site-packages (from transformers==2.9.0->layoutlm==0.0) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/site-packages (from transformers==2.9.0->layoutlm==0.0) (0.7.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/site-packages (from transformers==2.9.0->layoutlm==0.0) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/site-packages (from transformers==2.9.0->layoutlm==0.0) (4.50.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/site-packages (from transformers==2.9.0->layoutlm==0.0) (2020.10.11)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from tensorboardX==2.0->layoutlm==0.0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/site-packages (from tensorboardX==2.0->layoutlm==0.0) (3.13.0)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/site-packages (from seqeval==0.0.12->layoutlm==0.0) (2.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests->transformers==2.9.0->layoutlm==0.0) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/site-packages (from requests->transformers==2.9.0->layoutlm==0.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/site-packages (from requests->transformers==2.9.0->layoutlm==0.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests->transformers==2.9.0->layoutlm==0.0) (1.25.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/site-packages (from sacremoses->transformers==2.9.0->layoutlm==0.0) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/site-packages (from sacremoses->transformers==2.9.0->layoutlm==0.0) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorboardX==2.0->layoutlm==0.0) (50.3.0.post20201006)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval==0.0.12->layoutlm==0.0) (1.5.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval==0.0.12->layoutlm==0.0) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval==0.0.12->layoutlm==0.0) (5.3.1)\n",
            "Building wheels for collected packages: layoutlm\n",
            "  Building wheel for layoutlm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for layoutlm: filename=layoutlm-0.0-py3-none-any.whl size=208994 sha256=d953ea9673b53533110e837d0089176bbc484645b4ac5225d105eab3d182b422\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-q5gtc3un/wheels/18/32/63/e3fe0420420fe467f010d12e4b4af9b4b518cc3b2242a7bfb8\n",
            "Successfully built layoutlm\n",
            "Installing collected packages: layoutlm\n",
            "  Attempting uninstall: layoutlm\n",
            "    Found existing installation: layoutlm 0.0\n",
            "    Uninstalling layoutlm-0.0:\n",
            "      Successfully uninstalled layoutlm-0.0\n",
            "Successfully installed layoutlm-0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx7MMRC6L3Oi"
      },
      "source": [
        "%%bash\n",
        "source /usr/local/etc/profile.d/conda.sh\n",
        "conda activate layoutlm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0SK-9cIuhVn",
        "outputId": "3376b490-9b6b-41c0-f208-9de0832082ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "source": [
        "%cd \"/content/drive/My Drive/Colab Notebooks/unilm/layoutlm/examples/seq_labeling\"\n",
        "!python run_seq_labeling.py --data_dir data \\\n",
        "                            --model_type layoutlm \\\n",
        "                            --model_name_or_path ../layoutlm-base-uncased \\\n",
        "                            --do_lower_case \\\n",
        "                            --max_seq_length 512 \\\n",
        "                            --do_train \\\n",
        "                            --num_train_epochs 100.0 \\\n",
        "                            --logging_steps 10 \\\n",
        "                            --save_steps -1 \\\n",
        "                            --output_dir output7 \\\n",
        "                            --labels data/labels.txt \\\n",
        "                            --per_gpu_train_batch_size 16 \\\n",
        "                            --per_gpu_eval_batch_size 16 \\\n",
        "                            --fp16"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/unilm/layoutlm/examples/seq_labeling\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/site-packages/transformers/configuration_utils.py\", line 253, in get_config_dict\n",
            "    raise EnvironmentError\n",
            "OSError\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"run_seq_labeling.py\", line 811, in <module>\n",
            "    main()\n",
            "  File \"run_seq_labeling.py\", line 677, in main\n",
            "    cache_dir=args.cache_dir if args.cache_dir else None,\n",
            "  File \"/usr/local/lib/python3.6/site-packages/transformers/configuration_utils.py\", line 202, in from_pretrained\n",
            "    config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/site-packages/transformers/configuration_utils.py\", line 272, in get_config_dict\n",
            "    raise EnvironmentError(msg)\n",
            "OSError: Can't load '../layoutlm-base-uncased'. Make sure that:\n",
            "\n",
            "- '../layoutlm-base-uncased' is a correct model identifier listed on 'https://huggingface.co/models'\n",
            "\n",
            "- or '../layoutlm-base-uncased' is the correct path to a directory containing a 'config.json' file\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}